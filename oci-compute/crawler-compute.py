#!/usr/bin/env python3
"""
å°ä¸­å¸‚å»ºç…§çˆ¬èŸ² - OCI Compute Instance ç‰ˆæœ¬
æ¯æ—¥è‡ªå‹•çˆ¬å–æ–°çš„å»ºç…§è³‡æ–™ä¸¦ä¸Šå‚³åˆ° OCI Object Storage
"""
import json
import logging
import os
import subprocess
import time
from datetime import datetime
from pathlib import Path

import oci
from oci.object_storage import ObjectStorageClient

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/home/ubuntu/crawler.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# OCI è¨­å®š
NAMESPACE = "nrsdi1rz5vl8"
BUCKET_NAME = "taichung-building-permits"

class OCIStorage:
    def __init__(self):
        # ä½¿ç”¨ Instance Principal èªè­‰
        try:
            signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()
            self.object_storage = ObjectStorageClient(config={}, signer=signer)
            logger.info("ä½¿ç”¨ Instance Principal èªè­‰")
        except Exception:
            # å¦‚æœ Instance Principal å¤±æ•—ï¼Œä½¿ç”¨é è¨­è¨­å®šæª”
            config = oci.config.from_file()
            self.object_storage = ObjectStorageClient(config)
            logger.info("ä½¿ç”¨è¨­å®šæª”èªè­‰")
    
    def get_json_object(self, object_name):
        """å¾ OCI è®€å– JSON ç‰©ä»¶"""
        try:
            response = self.object_storage.get_object(NAMESPACE, BUCKET_NAME, object_name)
            return json.loads(response.data.content.decode('utf-8'))
        except Exception as e:
            logger.error(f"è®€å–ç‰©ä»¶å¤±æ•— {object_name}: {str(e)}")
            return None
    
    def put_json_object(self, object_name, data):
        """å¯«å…¥ JSON ç‰©ä»¶åˆ° OCI"""
        try:
            json_data = json.dumps(data, ensure_ascii=False, indent=2)
            self.object_storage.put_object(
                NAMESPACE, 
                BUCKET_NAME, 
                object_name,
                json_data.encode('utf-8')
            )
            logger.info(f"æˆåŠŸå¯«å…¥ {object_name}")
            return True
        except Exception as e:
            logger.error(f"å¯«å…¥ç‰©ä»¶å¤±æ•— {object_name}: {str(e)}")
            return False
    
    def get_current_progress(self):
        """å–å¾—ç›®å‰çˆ¬å–é€²åº¦"""
        permits = self.get_json_object('data/permits.json') or []
        
        progress = {
            'year114': {'count': 0, 'max': 0},
            'year113': {'count': 0, 'max': 0},
            'year112': {'count': 0, 'max': 0},
            'total': len(permits)
        }
        
        if isinstance(permits, list):
            for permit in permits:
                year = permit.get('permitYear', 0)
                seq = permit.get('sequenceNumber', 0)
                
                if year == 114:
                    progress['year114']['count'] += 1
                    progress['year114']['max'] = max(progress['year114']['max'], seq)
                elif year == 113:
                    progress['year113']['count'] += 1
                    progress['year113']['max'] = max(progress['year113']['max'], seq)
                elif year == 112:
                    progress['year112']['count'] += 1
                    progress['year112']['max'] = max(progress['year112']['max'], seq)
        
        return progress

class BuildingPermitCrawler:
    def __init__(self, storage):
        self.storage = storage
        self.base_url = "https://mcgbm.taichung.gov.tw/bupic/pages/queryInfoAction.do"
        self.max_consecutive_failures = 5
        self.max_consecutive_no_data = 20
        self.batch_size = 30
        self.request_delay = 0.8
        self.max_crawl_per_run = 50
        
        self.stats = {
            'totalAttempted': 0,
            'successful': 0,
            'failed': 0,
            'noData': 0
        }
        self.results = []
    
    def generate_index_key(self, year, permit_type, sequence, version=0):
        """ç”Ÿæˆ INDEX_KEY"""
        return f"{year}{permit_type}{sequence:05d}{version:02d}"
    
    def fetch_permit(self, index_key):
        """ä½¿ç”¨ wget æŠ“å–å»ºç…§è³‡æ–™"""
        url = f"{self.base_url}?INDEX_KEY={index_key}"
        
        try:
            # å»ºç«‹æš«å­˜æª”æ¡ˆ
            temp_file = f"/tmp/permit_{index_key}.html"
            
            # ä½¿ç”¨ wget ä¸‹è¼‰
            cmd = [
                'wget', '-q', '-O', temp_file,
                '--timeout=30',
                '--tries=3',
                '--wait-retry=2',
                '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                url
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0 and Path(temp_file).exists():
                with open(temp_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                # æ¸…ç†æš«å­˜æª”æ¡ˆ
                os.unlink(temp_file)
                return content
            else:
                if Path(temp_file).exists():
                    os.unlink(temp_file)
                logger.error(f"wget å¤±æ•—: {result.stderr}")
                return None
                
        except Exception as e:
            logger.error(f"æŠ“å–éŒ¯èª¤ {index_key}: {str(e)}")
            return None
    
    def parse_permit_data(self, html_content, index_key):
        """è§£æå»ºç…§è³‡æ–™"""
        try:
            import re
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«éºå¤±å€‹è³‡
            if 'â—‹â—‹â—‹ä»£è¡¨éºå¤±å€‹è³‡' in html_content:
                return "NO_DATA"
            
            permit_data = {
                'indexKey': index_key,
                'permitYear': int(index_key[0:3]),
                'permitType': int(index_key[3:4]),
                'sequenceNumber': int(index_key[4:9]),
                'versionNumber': int(index_key[9:11]),
                'crawledAt': datetime.now().isoformat()
            }
            
            # å»ºç…§è™Ÿç¢¼
            match = re.search(r'å»ºé€ åŸ·ç…§è™Ÿç¢¼[ï¼š:]\s*([^\s<]+)', html_content)
            if not match:
                match = re.search(r'å»ºç¯‰åŸ·ç…§è™Ÿç¢¼[ï¼š:]\s*([^\s<]+)', html_content)
            if match:
                permit_data['permitNumber'] = match.group(1).strip()
            else:
                return None
            
            # ç”³è«‹äºº
            match = re.search(r'èµ·é€ äºº[^>]*å§“å[^>]*>([^<]+)', html_content)
            if match:
                permit_data['applicantName'] = match.group(1).strip()
            
            # åœ°å€
            match = re.search(r'åœ°è™Ÿ[^>]*>([^<]+)', html_content)
            if not match:
                match = re.search(r'åœ°å€[^>]*>([^<]+)', html_content)
            if match:
                permit_data['siteAddress'] = match.group(1).strip()
            
            # è¡Œæ”¿å€
            if permit_data.get('siteAddress'):
                district_match = re.search(r'è‡ºä¸­å¸‚([^å€]+å€)', permit_data['siteAddress'])
                if district_match:
                    permit_data['district'] = district_match.group(1)
            
            # æ¨“å±¤è³‡è¨Š
            match = re.search(r'åœ°ä¸Š.*?å±¤.*?æ£Ÿ.*?æˆ¶|åœ°ä¸Š.*?å±¤.*?å¹¢.*?æ£Ÿ.*?æˆ¶', html_content)
            if match:
                permit_data['floorInfo'] = match.group(0)
                
                floor_match = re.search(r'åœ°ä¸Š(\d+)å±¤', match.group(0))
                if floor_match:
                    permit_data['floorsAbove'] = int(floor_match.group(1))
                    permit_data['floors'] = int(floor_match.group(1))
                
                building_match = re.search(r'(\d+)æ£Ÿ', match.group(0))
                if building_match:
                    permit_data['buildingCount'] = int(building_match.group(1))
                    permit_data['buildings'] = int(building_match.group(1))
                
                unit_match = re.search(r'(\d+)æˆ¶', match.group(0))
                if unit_match:
                    permit_data['unitCount'] = int(unit_match.group(1))
                    permit_data['units'] = int(unit_match.group(1))
            
            # ç¸½æ¨“åœ°æ¿é¢ç©
            match = re.search(r'ç¸½æ¨“åœ°æ¿é¢ç©.*?<span[^>]*>([0-9.,]+)', html_content)
            if match:
                permit_data['totalFloorArea'] = float(match.group(1).replace(',', ''))
            
            # ç™¼ç…§æ—¥æœŸ
            match = re.search(r'ç™¼ç…§æ—¥æœŸ.*?(\d{3})/(\d{2})/(\d{2})', html_content)
            if match:
                year = int(match.group(1))
                month = int(match.group(2))
                day = int(match.group(3))
                permit_data['issueDate'] = f"{year + 1911}-{month:02d}-{day:02d}"
                permit_data['issueDateROC'] = f"{match.group(1)}/{match.group(2)}/{match.group(3)}"
            
            return permit_data
            
        except Exception as e:
            logger.error(f'è§£æéŒ¯èª¤: {str(e)}')
            return None
    
    def crawl_year_range(self, year, start_seq, max_crawl=None):
        """çˆ¬å–å¹´ä»½ç¯„åœ"""
        if max_crawl is None:
            max_crawl = self.max_crawl_per_run
            
        logger.info(f"é–‹å§‹çˆ¬å– {year} å¹´ï¼Œå¾åºè™Ÿ {start_seq:05d} é–‹å§‹")
        logger.info(f"åƒæ•¸: å»¶é²={self.request_delay}s, æ‰¹æ¬¡={self.batch_size}, ä¸Šé™={max_crawl}")
        
        consecutive_failed = 0
        consecutive_no_data = 0
        seq = start_seq
        
        while consecutive_failed < self.max_consecutive_failures and self.stats['totalAttempted'] < max_crawl:
            index_key = self.generate_index_key(year, 1, seq)
            self.stats['totalAttempted'] += 1
            
            logger.info(f"[{seq:05d}] çˆ¬å– {index_key}...")
            
            html_content = self.fetch_permit(index_key)
            if not html_content:
                consecutive_failed += 1
                consecutive_no_data = 0
                self.stats['failed'] += 1
                logger.info(f"âŒ {index_key} å¤±æ•— (é€£çºŒ {consecutive_failed})")
            else:
                result = self.parse_permit_data(html_content, index_key)
                if result == "NO_DATA":
                    consecutive_no_data += 1
                    consecutive_failed = 0
                    self.stats['noData'] += 1
                    logger.info(f"â­ï¸ {index_key} ç„¡è³‡æ–™ (é€£çºŒ {consecutive_no_data})")
                    
                    if consecutive_no_data >= self.max_consecutive_no_data:
                        logger.info(f"é€£çºŒ {self.max_consecutive_no_data} ç­†ç„¡è³‡æ–™ï¼Œåœæ­¢çˆ¬å–")
                        break
                elif result:
                    consecutive_failed = 0
                    consecutive_no_data = 0
                    self.stats['successful'] += 1
                    self.results.append(result)
                    logger.info(f"âœ… {index_key} {result.get('permitNumber', '')}")
                else:
                    consecutive_failed += 1
                    consecutive_no_data = 0
                    self.stats['failed'] += 1
                    logger.info(f"âŒ {index_key} è§£æå¤±æ•— (é€£çºŒ {consecutive_failed})")
            
            # æ‰¹æ¬¡å„²å­˜
            if len(self.results) >= self.batch_size:
                logger.info(f"æ‰¹æ¬¡å„²å­˜ {len(self.results)} ç­†è³‡æ–™...")
                self.save_results()
            
            # å»¶é²é¿å…éå¿«
            time.sleep(self.request_delay)
            seq += 1
        
        # å„²å­˜å‰©é¤˜çµæœ
        if self.results:
            self.save_results()
        
        return self.stats
    
    def save_results(self):
        """å„²å­˜çˆ¬å–çµæœåˆ° OCI"""
        if not self.results:
            return
        
        # è®€å–ç¾æœ‰è³‡æ–™
        permits = self.storage.get_json_object('data/permits.json') or []
        
        # å»ºç«‹ç´¢å¼•å­—å…¸ä»¥å¿«é€ŸæŸ¥æ‰¾
        existing_keys = {p['indexKey'] for p in permits}
        
        # æ–°å¢ä¸é‡è¤‡çš„è³‡æ–™
        new_count = 0
        for result in self.results:
            if result['indexKey'] not in existing_keys:
                permits.append(result)
                new_count += 1
        
        # é‡æ–°æ’åº
        permits.sort(key=lambda x: x['indexKey'])
        
        # å„²å­˜å› OCI
        if self.storage.put_json_object('data/permits.json', permits):
            logger.info(f"âœ… å„²å­˜ {new_count} ç­†æ–°è³‡æ–™ï¼ˆç¸½è¨ˆ {len(permits)} ç­†ï¼‰")
        
        self.results = []
    
    def run_daily_crawl(self):
        """åŸ·è¡Œæ¯æ—¥çˆ¬èŸ²"""
        start_time = datetime.now()
        logger.info("="*70)
        logger.info(f"ğŸš€ é–‹å§‹æ¯æ—¥çˆ¬èŸ²: {start_time}")
        logger.info("="*70)
        
        try:
            # å–å¾—ç›®å‰é€²åº¦
            progress = self.storage.get_current_progress()
            logger.info('ğŸ“Š ç›®å‰é€²åº¦:')
            logger.info(f"  114å¹´: {progress['year114']['count']} ç­†, æœ€å¤§åºè™Ÿ: {progress['year114']['max']}")
            logger.info(f"  113å¹´: {progress['year113']['count']} ç­†, æœ€å¤§åºè™Ÿ: {progress['year113']['max']}")
            logger.info(f"  112å¹´: {progress['year112']['count']} ç­†, æœ€å¤§åºè™Ÿ: {progress['year112']['max']}")
            logger.info(f"  ç¸½è¨ˆ: {progress['total']} ç­†")
            
            # æ±ºå®šè¦çˆ¬å–çš„å¹´ä»½å’Œèµ·å§‹åºè™Ÿ
            if progress['year114']['max'] < 2000:  # å‡è¨­ 114 å¹´æœ€å¤šåˆ° 2000
                target_year = 114
                start_seq = progress['year114']['max'] + 1
            elif progress['year113']['max'] < 2500:  # å‡è¨­ 113 å¹´æœ€å¤šåˆ° 2500
                target_year = 113
                start_seq = progress['year113']['max'] + 1
            elif progress['year112']['max'] < 2500:  # å‡è¨­ 112 å¹´æœ€å¤šåˆ° 2500
                target_year = 112
                start_seq = progress['year112']['max'] + 1
            else:
                logger.info("âœ… æ‰€æœ‰å¹´ä»½éƒ½å·²çˆ¬å–å®Œæˆ")
                return
            
            logger.info(f"\nğŸ“Š é–‹å§‹çˆ¬å–: {target_year}å¹´ åºè™Ÿ{start_seq}")
            
            # åŸ·è¡Œçˆ¬å–
            stats = self.crawl_year_range(target_year, start_seq)
            
            # è¨˜éŒ„æ—¥èªŒ
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            log_entry = {
                'date': start_time.strftime('%Y-%m-%d'),
                'startTime': start_time.isoformat(),
                'endTime': end_time.isoformat(),
                'duration': int(duration),
                'results': {
                    'success': stats['successful'],
                    'failed': stats['failed'],
                    'empty': stats['noData'],
                    'total': stats['totalAttempted']
                },
                'yearStats': {
                    str(target_year): {
                        'crawled': stats['successful'],
                        'start': start_seq,
                        'end': start_seq + stats['totalAttempted'] - 1
                    }
                },
                'status': 'completed' if stats['successful'] > 0 else 'partial'
            }
            
            # æ›´æ–°æ—¥èªŒ
            logs_data = self.storage.get_json_object('data/crawl-logs.json') or {'logs': []}
            logs_data['logs'].insert(0, log_entry)
            logs_data['logs'] = logs_data['logs'][:100]  # ä¿ç•™æœ€è¿‘ 100 ç­†
            logs_data['lastUpdate'] = datetime.now().isoformat()
            self.storage.put_json_object('data/crawl-logs.json', logs_data)
            
            logger.info("="*70)
            logger.info(f"ğŸ çˆ¬èŸ²å®Œæˆ")
            logger.info(f"ğŸ“Š çµ±è¨ˆ: ç¸½å˜—è©¦ {stats['totalAttempted']} ç­†")
            logger.info(f"   âœ… æˆåŠŸ: {stats['successful']} ç­†")
            logger.info(f"   âŒ å¤±æ•—: {stats['failed']} ç­†")
            logger.info(f"   â­ï¸ ç„¡è³‡æ–™: {stats['noData']} ç­†")
            logger.info(f"â±ï¸ è€—æ™‚: {duration:.0f} ç§’")
            logger.info("="*70)
            
        except Exception as e:
            logger.error(f"çˆ¬èŸ²åŸ·è¡ŒéŒ¯èª¤: {str(e)}", exc_info=True)
            
            # è¨˜éŒ„éŒ¯èª¤
            log_entry = {
                'date': start_time.strftime('%Y-%m-%d'),
                'startTime': start_time.isoformat(),
                'endTime': datetime.now().isoformat(),
                'error': str(e),
                'status': 'failed'
            }
            
            logs_data = self.storage.get_json_object('data/crawl-logs.json') or {'logs': []}
            logs_data['logs'].insert(0, log_entry)
            logs_data['lastUpdate'] = datetime.now().isoformat()
            self.storage.put_json_object('data/crawl-logs.json', logs_data)

def main():
    """ä¸»ç¨‹å¼"""
    storage = OCIStorage()
    crawler = BuildingPermitCrawler(storage)
    crawler.run_daily_crawl()

if __name__ == "__main__":
    main()
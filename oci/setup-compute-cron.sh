#!/bin/bash
# åœ¨ OCI Compute Instance ä¸Šè¨­å®šè‡ªå‹•çˆ¬èŸ²

echo "======================================="
echo "OCI Compute Instance è‡ªå‹•çˆ¬èŸ²è¨­å®š"
echo "======================================="
echo ""
echo "é€™å€‹è…³æœ¬æ‡‰è©²åœ¨æ‚¨çš„ OCI Compute Instance ä¸ŠåŸ·è¡Œ"
echo ""

# å»ºç«‹çˆ¬èŸ²ç›®éŒ„
mkdir -p ~/building-permit-crawler
cd ~/building-permit-crawler

# ä¸‹è¼‰çˆ¬èŸ²è…³æœ¬
echo "ğŸ“¥ ä¸‹è¼‰çˆ¬èŸ²è…³æœ¬..."
oci os object get \
    --namespace nrsdi1rz5vl8 \
    --bucket-name taichung-building-permits \
    --name scripts/oci-crawler.py \
    --file oci-crawler.py

chmod +x oci-crawler.py

# å»ºç«‹æ¯æ—¥åŸ·è¡Œè…³æœ¬
cat > daily-crawler.sh << 'EOF'
#!/bin/bash
# æ¯æ—¥åŸ·è¡Œå»ºç…§çˆ¬èŸ²

LOG_FILE="/home/opc/building-permit-crawler/logs/crawler-$(date +%Y%m%d).log"
mkdir -p /home/opc/building-permit-crawler/logs

echo "========================================" >> $LOG_FILE
echo "é–‹å§‹åŸ·è¡Œå»ºç…§çˆ¬èŸ²: $(date)" >> $LOG_FILE
echo "========================================" >> $LOG_FILE

# åŸ·è¡Œçˆ¬èŸ²
cd /home/opc/building-permit-crawler
/usr/bin/python3 oci-crawler.py >> $LOG_FILE 2>&1

echo "çˆ¬èŸ²åŸ·è¡Œå®Œæˆ: $(date)" >> $LOG_FILE
echo "" >> $LOG_FILE

# æ¸…ç†30å¤©å‰çš„æ—¥èªŒ
find /home/opc/building-permit-crawler/logs -name "*.log" -mtime +30 -delete
EOF

chmod +x daily-crawler.sh

# å»ºç«‹ç°¡åŒ–ç‰ˆçˆ¬èŸ²ï¼ˆåªçˆ¬æœ€æ–°è³‡æ–™ï¼‰
cat > daily-update-crawler.py << 'EOF'
#!/usr/bin/env python3
"""
æ¯æ—¥æ›´æ–°çˆ¬èŸ² - åªçˆ¬å–æœ€æ–°çš„å»ºç…§è³‡æ–™
"""

import subprocess
import json
import re
from datetime import datetime
import time
import logging

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def main():
    base_url = "https://mcgbm.taichung.gov.tw/bupic/pages/queryInfoAction.do"
    namespace = "nrsdi1rz5vl8"
    bucket_name = "taichung-building-permits"
    
    # è¼‰å…¥ç¾æœ‰è³‡æ–™
    logger.info("è¼‰å…¥ç¾æœ‰è³‡æ–™...")
    permits = []
    try:
        subprocess.run(["oci", "os", "object", "get", "--namespace", namespace,
                       "--bucket-name", bucket_name, "--name", "data/permits.json",
                       "--file", "/tmp/existing.json"], capture_output=True)
        with open('/tmp/existing.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            permits = data.get('permits', [])
            logger.info(f"è¼‰å…¥ {len(permits)} ç­†ç¾æœ‰è³‡æ–™")
    except:
        logger.error("ç„¡æ³•è¼‰å…¥ç¾æœ‰è³‡æ–™")
        return
    
    # å–å¾—ç•¶å‰å¹´ä»½ï¼ˆæ°‘åœ‹å¹´ï¼‰
    current_year = datetime.now().year - 1911
    
    # çˆ¬å–æœ€æ–°50ç­†
    new_count = 0
    max_crawl = 50
    permit_type = 1
    
    logger.info(f"é–‹å§‹çˆ¬å– {current_year} å¹´æœ€æ–°è³‡æ–™...")
    
    # å¾å¤§åºè™Ÿå¾€å›çˆ¬
    for seq in range(9999, 0, -1):
        if new_count >= max_crawl:
            break
            
        index_key = f"{current_year}{permit_type}{seq:05d}00"
        
        # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if any(p.get('indexKey') == index_key for p in permits):
            continue
        
        # çˆ¬å–å–®ç­†
        try:
            # ç¬¬ä¸€æ¬¡è¨ªå•
            subprocess.run(["wget", "-q", "--save-cookies=/tmp/cookies.txt",
                          "--user-agent=Mozilla/5.0", "-O", "/tmp/first.html",
                          f"{base_url}?INDEX_KEY={index_key}"], capture_output=True)
            
            time.sleep(3)
            
            # ç¬¬äºŒæ¬¡è¨ªå•
            result = subprocess.run(["wget", "-q", "--load-cookies=/tmp/cookies.txt",
                                   "--user-agent=Mozilla/5.0", "-O", f"/tmp/page.html",
                                   f"{base_url}?INDEX_KEY={index_key}"], capture_output=True)
            
            if result.returncode == 0:
                with open("/tmp/page.html", "rb") as f:
                    content = f.read()
                
                if len(content) > 1000:
                    try:
                        html = content.decode('big5')
                    except:
                        html = content.decode('utf-8', errors='ignore')
                    
                    # æª¢æŸ¥æ˜¯å¦æœ‰å»ºç…§è³‡æ–™
                    if "å»ºé€ åŸ·ç…§è™Ÿç¢¼" in html:
                        # ç°¡å–®è§£æ
                        permit_data = {'indexKey': index_key, 'crawledAt': datetime.now().isoformat()}
                        
                        # å»ºç…§è™Ÿç¢¼
                        m = re.search(r'<span class="conlist w20 tc">([^<]+è™Ÿ)</span>', html)
                        if m:
                            permit_data['permitNumber'] = m.group(1)
                            permit_data['permitYear'] = current_year
                            permit_data['permitType'] = permit_type
                            permit_data['sequenceNumber'] = seq
                            permit_data['versionNumber'] = 0
                            
                            permits.append(permit_data)
                            new_count += 1
                            logger.info(f"âœ… æ–°å¢: {permit_data['permitNumber']}")
                        
        except Exception as e:
            logger.error(f"çˆ¬å–å¤±æ•— {index_key}: {e}")
        
        time.sleep(2)
    
    # å„²å­˜æ›´æ–°å¾Œçš„è³‡æ–™
    if new_count > 0:
        logger.info(f"å„²å­˜ {new_count} ç­†æ–°è³‡æ–™...")
        
        sorted_permits = sorted(permits, key=lambda x: (
            -x.get('permitYear', 0),
            -x.get('sequenceNumber', 0)
        ))
        
        data = {
            "lastUpdate": datetime.now().isoformat(),
            "totalCount": len(sorted_permits),
            "yearCounts": {},
            "permits": sorted_permits
        }
        
        for permit in sorted_permits:
            year = permit.get('permitYear', 0)
            if year not in data['yearCounts']:
                data['yearCounts'][year] = 0
            data['yearCounts'][year] += 1
        
        with open('/tmp/permits_updated.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        subprocess.run(["oci", "os", "object", "put", "--namespace", namespace,
                       "--bucket-name", bucket_name, "--name", "data/permits.json",
                       "--file", "/tmp/permits_updated.json", "--force"], capture_output=True)
        
        logger.info("âœ… è³‡æ–™æ›´æ–°å®Œæˆ")
    else:
        logger.info("æ²’æœ‰ç™¼ç¾æ–°è³‡æ–™")

if __name__ == "__main__":
    main()
EOF

chmod +x daily-update-crawler.py

# è¨­å®š crontab
echo ""
echo "ğŸ“… è¨­å®š crontab..."
echo ""

# å»ºç«‹ crontab é …ç›®
CRON_CMD="/home/opc/building-permit-crawler/daily-crawler.sh"
CRON_TIME="0 3 * * *"  # æ¯å¤©å‡Œæ™¨ 3:00

# æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
if crontab -l 2>/dev/null | grep -q "$CRON_CMD"; then
    echo "âœ… Crontab å·²ç¶“è¨­å®š"
else
    # æ·»åŠ åˆ° crontab
    (crontab -l 2>/dev/null; echo "$CRON_TIME $CRON_CMD") | crontab -
    echo "âœ… Crontab è¨­å®šå®Œæˆ"
fi

echo ""
echo "======================================="
echo "âœ… è‡ªå‹•çˆ¬èŸ²è¨­å®šå®Œæˆï¼"
echo "======================================="
echo ""
echo "è¨­å®šå…§å®¹ï¼š"
echo "- æ¯æ—¥å‡Œæ™¨ 3:00 è‡ªå‹•åŸ·è¡Œ"
echo "- çˆ¬å–æœ€æ–° 50 ç­†å»ºç…§è³‡æ–™"
echo "- æ—¥èªŒä¿å­˜åœ¨ ~/building-permit-crawler/logs/"
echo ""
echo "æ¸¬è©¦å‘½ä»¤ï¼š"
echo "  ./daily-crawler.sh"
echo ""
echo "æŸ¥çœ‹ crontabï¼š"
echo "  crontab -l"
echo ""
echo "æŸ¥çœ‹æœ€æ–°æ—¥èªŒï¼š"
echo "  tail -f ~/building-permit-crawler/logs/crawler-*.log"
echo ""
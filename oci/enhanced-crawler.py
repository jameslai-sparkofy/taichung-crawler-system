#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°ä¸­å¸‚å»ºç…§çˆ¬èŸ² - å…·å‚™å³æ™‚å¯¶ä½³æ©Ÿæ§‹ç¯©é¸åŠŸèƒ½
"""

# è¼‰å…¥åŸå§‹çˆ¬èŸ²
exec(open('optimized-crawler-stable.py').read().split('if __name__ == "__main__":')[0])

# è¼‰å…¥å³æ™‚ç¯©é¸å™¨
from baojia_realtime_filter import BaojiaRealtimeFilter

class EnhancedCrawler(OptimizedCrawler):
    def __init__(self):
        super().__init__()
        self.baojia_filter = BaojiaRealtimeFilter()
    
    def crawl_single_permit(self, index_key):
        """çˆ¬å–å–®ç­†å»ºç…§ä¸¦å³æ™‚æ¨™è¨˜æ˜¯å¦ç‚ºå¯¶ä½³æ©Ÿæ§‹"""
        result = super().crawl_single_permit(index_key)
        
        if result and isinstance(result, dict):
            # å³æ™‚åˆ¤æ–·æ˜¯å¦ç‚ºå¯¶ä½³æ©Ÿæ§‹
            applicant = result.get('applicantName', '')
            if applicant:
                result['isBaojia'] = self.baojia_filter.is_baojia_company(applicant)
                if result['isBaojia']:
                    print(f"  ğŸ¢ å¯¶ä½³æ©Ÿæ§‹å»ºæ¡ˆ: {applicant}")
        
        return result
    
    def upload_batch_data(self, batch_data):
        """ä¸Šå‚³è³‡æ–™ä¸¦åŒæ™‚æ›´æ–°å¯¶ä½³ç¯©é¸çµæœ"""
        # å…ˆåŸ·è¡ŒåŸæœ¬çš„ä¸Šå‚³
        success = super().upload_batch_data(batch_data)
        
        if success:
            # å³æ™‚æ›´æ–°å¯¶ä½³ç¯©é¸çµæœ
            self._update_baojia_filter_results()
        
        return success
    
    def _update_baojia_filter_results(self):
        """æ›´æ–°å¯¶ä½³ç¯©é¸çµæœåˆ°OCI"""
        try:
            # ä¸‹è¼‰æœ€æ–°å»ºç…§è³‡æ–™
            subprocess.run([
                'oci', 'os', 'object', 'get',
                '--namespace', self.namespace,
                '--bucket-name', self.bucket_name,
                '--name', 'data/permits.json',
                '--file', '/tmp/permits_for_filter.json'
            ], capture_output=True)
            
            with open('/tmp/permits_for_filter.json', 'r') as f:
                data = json.load(f)
            
            # å³æ™‚ç¯©é¸
            filter_result = self.baojia_filter.filter_permits_realtime(data.get('permits', []))
            
            # å„²å­˜ç¯©é¸çµæœ
            with open('/tmp/baojia_realtime_results.json', 'w') as f:
                json.dump(filter_result, f, ensure_ascii=False, indent=2)
            
            # ä¸Šå‚³åˆ°OCI
            subprocess.run([
                'oci', 'os', 'object', 'put',
                '--namespace', self.namespace,
                '--bucket-name', self.bucket_name,
                '--name', 'data/baojia_realtime_results.json',
                '--file', '/tmp/baojia_realtime_results.json',
                '--force'
            ], capture_output=True)
            
            print(f"  ğŸ“Š å¯¶ä½³ç¯©é¸çµæœå·²æ›´æ–°: {filter_result['totalCount']} ç­†")
            
        except Exception as e:
            print(f"  âš ï¸ æ›´æ–°å¯¶ä½³ç¯©é¸çµæœå¤±æ•—: {e}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 3:
        print("ä½¿ç”¨æ–¹å¼: python enhanced-crawler.py <å¹´ä»½> <èµ·å§‹åºè™Ÿ> [çµæŸåºè™Ÿ]")
        sys.exit(1)
    
    year = int(sys.argv[1])
    start_seq = int(sys.argv[2])
    end_seq = int(sys.argv[3]) if len(sys.argv) > 3 else None
    
    crawler = EnhancedCrawler()
    
    if end_seq:
        print(f"é–‹å§‹çˆ¬å– {year} å¹´å»ºç…§è³‡æ–™ï¼Œåºè™Ÿ {start_seq} åˆ° {end_seq}")
        crawler.crawl_year_range(year, start_seq, end_seq)
    else:
        print(f"é–‹å§‹çˆ¬å– {year} å¹´å»ºç…§è³‡æ–™ï¼Œå¾åºè™Ÿ {start_seq} é–‹å§‹")
        crawler.crawl_year_continuous(year, start_seq)

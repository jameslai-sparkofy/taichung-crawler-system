#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¾©114å¹´ç©ºç™½è³‡æ–™
"""

import json
import subprocess
import time
from datetime import datetime

# è¼‰å…¥çˆ¬èŸ²
exec(open('optimized-crawler.py').read().split('if __name__ == "__main__":')[0])

def find_empty_data_sequences():
    """æ‰¾å‡ºç©ºç™½è³‡æ–™çš„åºè™Ÿ"""
    print("ğŸ“¥ è¼‰å…¥ç¾æœ‰è³‡æ–™...")
    
    # ä¸‹è¼‰æœ€æ–°è³‡æ–™
    subprocess.run([
        'oci', 'os', 'object', 'get',
        '--namespace', 'nrsdi1rz5vl8',
        '--bucket-name', 'taichung-building-permits',
        '--name', 'data/permits.json',
        '--file', '/tmp/check_permits.json'
    ], capture_output=True)
    
    with open('/tmp/check_permits.json', 'r') as f:
        data = json.load(f)
    
    # æ‰¾å‡º114å¹´ç©ºç™½è³‡æ–™
    empty_sequences = []
    for p in data['permits']:
        if p.get('permitYear') == 114:
            seq = p.get('sequenceNumber', 0)
            # æª¢æŸ¥æ˜¯å¦ç‚ºç©ºç™½è³‡æ–™
            if not p.get('applicantName', '').strip() or not p.get('siteAddress', '').strip():
                empty_sequences.append(seq)
    
    print(f"æ‰¾åˆ° {len(empty_sequences)} ç­†ç©ºç™½è³‡æ–™")
    return sorted(empty_sequences)

def crawl_specific_sequences(sequences):
    """çˆ¬å–ç‰¹å®šåºè™Ÿ"""
    crawler = OptimizedCrawler()
    crawler.request_delay = 1.0
    crawler.batch_size = 20
    
    print(f"\nğŸ”§ é–‹å§‹ä¿®å¾© {len(sequences)} ç­†ç©ºç™½è³‡æ–™")
    
    # åˆ†æ‰¹è™•ç†
    batch = []
    success_count = 0
    
    for i, seq in enumerate(sequences):
        print(f"\n[{i+1}/{len(sequences)}] çˆ¬å–åºè™Ÿ {seq:05d}...")
        
        try:
            # ä½¿ç”¨åŸå§‹çˆ¬èŸ²çš„crawl_single_permitæ–¹æ³•
            permit_data = crawler.crawl_single_permit(114, seq)
            
            if permit_data and permit_data.get('applicantName'):
                batch.append(permit_data)
                success_count += 1
                print(f"   âœ… æˆåŠŸ: {permit_data['permitNumber']}")
                print(f"   ç”³è«‹äºº: {permit_data['applicantName']}")
                print(f"   åœ°å€: {permit_data['siteAddress'][:30]}...")
            else:
                print(f"   âš ï¸ ç„¡è³‡æ–™")
            
            # æ¯20ç­†ä¸Šå‚³ä¸€æ¬¡
            if len(batch) >= 20:
                print(f"\nğŸ’¾ ä¸Šå‚³ {len(batch)} ç­†è³‡æ–™...")
                if crawler.upload_batch_data(batch):
                    print("   âœ… ä¸Šå‚³æˆåŠŸ")
                    batch = []
                else:
                    print("   âŒ ä¸Šå‚³å¤±æ•—")
            
            time.sleep(crawler.request_delay)
            
        except Exception as e:
            print(f"   âŒ éŒ¯èª¤: {e}")
    
    # ä¸Šå‚³å‰©é¤˜è³‡æ–™
    if batch:
        print(f"\nğŸ’¾ ä¸Šå‚³æœ€å¾Œ {len(batch)} ç­†è³‡æ–™...")
        crawler.upload_batch_data(batch)
    
    print(f"\nâœ… å®Œæˆï¼æˆåŠŸä¿®å¾© {success_count}/{len(sequences)} ç­†è³‡æ–™")

if __name__ == "__main__":
    # æ‰¾å‡ºç©ºç™½è³‡æ–™
    empty_seqs = find_empty_data_sequences()
    
    if empty_seqs:
        # é¡¯ç¤ºå‰20å€‹
        print("\nç©ºç™½è³‡æ–™åºè™Ÿç¯„ä¾‹:")
        for seq in empty_seqs[:20]:
            print(f"  {seq}")
        if len(empty_seqs) > 20:
            print(f"  ... é‚„æœ‰ {len(empty_seqs) - 20} å€‹")
        
        # é–‹å§‹çˆ¬å–
        crawl_specific_sequences(empty_seqs)
    else:
        print("âœ… æ²’æœ‰æ‰¾åˆ°ç©ºç™½è³‡æ–™ï¼")
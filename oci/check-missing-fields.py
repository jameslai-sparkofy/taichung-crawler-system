#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æª¢æŸ¥ä¸¦é‡æ–°çˆ¬å–ç¼ºå°‘å¿…è¦æ¬„ä½çš„è³‡æ–™
"""

import json
import subprocess
import time
from datetime import datetime

# è¼‰å…¥çˆ¬èŸ²
exec(open('optimized-crawler-stable.py').read().split('if __name__ == "__main__":')[0])

def check_missing_fields():
    """æª¢æŸ¥ç¼ºå°‘å¿…è¦æ¬„ä½çš„è³‡æ–™"""
    print("ğŸ“¥ ä¸‹è¼‰æœ€æ–°è³‡æ–™...")
    subprocess.run([
        'oci', 'os', 'object', 'get',
        '--namespace', 'nrsdi1rz5vl8',
        '--bucket-name', 'taichung-building-permits',
        '--name', 'data/permits.json',
        '--file', '/tmp/check_fields.json'
    ], capture_output=True)
    
    with open('/tmp/check_fields.json', 'r') as f:
        data = json.load(f)
    
    # å¿…è¦æ¬„ä½
    required_fields = ['floors', 'buildings', 'units', 'totalFloorArea', 'issueDate']
    
    # çµ±è¨ˆå„å¹´ä»½çš„å•é¡Œè³‡æ–™
    missing_data = {}
    total_issues = 0
    
    for permit in data['permits']:
        year = permit.get('permitYear', 0)
        if year not in missing_data:
            missing_data[year] = []
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ç”³è«‹äººè³‡æ–™
        if not permit.get('applicantName', '').strip():
            continue  # è·³éå®Œå…¨ç©ºç™½çš„è³‡æ–™
        
        # æª¢æŸ¥å¿…è¦æ¬„ä½
        missing_fields = []
        for field in required_fields:
            if not permit.get(field):
                missing_fields.append(field)
        
        if missing_fields:
            total_issues += 1
            missing_data[year].append({
                'permitNumber': permit.get('permitNumber'),
                'sequenceNumber': permit.get('sequenceNumber', 0),
                'indexKey': permit.get('indexKey'),
                'missing': missing_fields
            })
    
    # é¡¯ç¤ºçµ±è¨ˆ
    print(f"\nğŸ“Š å¿…è¦æ¬„ä½ç¼ºå¤±çµ±è¨ˆ:")
    print(f"ç¸½è¨ˆæœ‰ {total_issues} ç­†è³‡æ–™ç¼ºå°‘å¿…è¦æ¬„ä½\n")
    
    for year in sorted(missing_data.keys(), reverse=True):
        if missing_data[year]:
            print(f"{year}å¹´: {len(missing_data[year])} ç­†")
            # é¡¯ç¤ºå‰5ç­†ç¯„ä¾‹
            for i, item in enumerate(missing_data[year][:5]):
                print(f"  - {item['permitNumber']}: ç¼ºå°‘ {', '.join(item['missing'])}")
            if len(missing_data[year]) > 5:
                print(f"  ... é‚„æœ‰ {len(missing_data[year]) - 5} ç­†")
            print()
    
    return missing_data

def recrawl_missing_fields(missing_data):
    """é‡æ–°çˆ¬å–ç¼ºå°‘æ¬„ä½çš„è³‡æ–™"""
    crawler = OptimizedCrawler()
    crawler.request_delay = 1.0
    crawler.batch_size = 30
    
    total_to_crawl = sum(len(items) for items in missing_data.values())
    print(f"\nğŸ”§ é–‹å§‹é‡æ–°çˆ¬å– {total_to_crawl} ç­†ç¼ºå°‘æ¬„ä½çš„è³‡æ–™")
    print("=" * 70)
    
    count = 0
    batch = []
    
    for year in sorted(missing_data.keys(), reverse=True):
        if not missing_data[year]:
            continue
            
        print(f"\nğŸ“… è™•ç† {year}å¹´è³‡æ–™ ({len(missing_data[year])} ç­†)")
        
        for item in missing_data[year]:
            count += 1
            index_key = item['indexKey']
            
            print(f"\n[{count}/{total_to_crawl}] {item['permitNumber']}")
            print(f"  ç¼ºå°‘: {', '.join(item['missing'])}")
            
            try:
                result = crawler.crawl_single_permit(index_key)
                
                if result and isinstance(result, dict):
                    # æª¢æŸ¥æ˜¯å¦æˆåŠŸç²å¾—æ‰€æœ‰å¿…è¦æ¬„ä½
                    still_missing = []
                    for field in item['missing']:
                        if not result.get(field):
                            still_missing.append(field)
                    
                    if not still_missing:
                        batch.append(result)
                        print(f"  âœ… æˆåŠŸè£œé½Šæ‰€æœ‰æ¬„ä½")
                    else:
                        print(f"  âš ï¸ ä»ç¼ºå°‘: {', '.join(still_missing)}")
                elif result == "NO_DATA":
                    print(f"  âš ï¸ ç„¡è³‡æ–™")
                else:
                    print(f"  âŒ çˆ¬å–å¤±æ•—")
                
                # æ‰¹æ¬¡ä¸Šå‚³
                if len(batch) >= crawler.batch_size:
                    print(f"\nğŸ’¾ ä¸Šå‚³ {len(batch)} ç­†è³‡æ–™...")
                    if crawler.upload_batch_data(batch):
                        print("  âœ… ä¸Šå‚³æˆåŠŸ")
                        batch = []
                    else:
                        print("  âŒ ä¸Šå‚³å¤±æ•—")
                
                time.sleep(crawler.request_delay)
                
            except Exception as e:
                print(f"  âŒ éŒ¯èª¤: {e}")
    
    # ä¸Šå‚³å‰©é¤˜è³‡æ–™
    if batch:
        print(f"\nğŸ’¾ ä¸Šå‚³æœ€å¾Œ {len(batch)} ç­†è³‡æ–™...")
        crawler.upload_batch_data(batch)
    
    print(f"\nâœ… é‡æ–°çˆ¬å–å®Œæˆ")

if __name__ == "__main__":
    # æª¢æŸ¥ç¼ºå¤±æ¬„ä½
    missing_data = check_missing_fields()
    
    # è©¢å•æ˜¯å¦è¦é‡æ–°çˆ¬å–
    if missing_data and any(missing_data.values()):
        print("\næ˜¯å¦è¦é‡æ–°çˆ¬å–é€™äº›è³‡æ–™? (y/n): ", end='')
        # è‡ªå‹•åŸ·è¡Œ
        print("y")
        recrawl_missing_fields(missing_data)
    else:
        print("\nâœ… æ‰€æœ‰è³‡æ–™éƒ½æœ‰å®Œæ•´çš„å¿…è¦æ¬„ä½ï¼")
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é‡æ–°çˆ¬å–114å¹´ç©ºç™½è³‡æ–™
"""

import json
import subprocess
import time

# è¼‰å…¥çˆ¬èŸ²
exec(open('optimized-crawler.py').read().split('if __name__ == "__main__":')[0])

# ä¸‹è¼‰æœ€æ–°è³‡æ–™
print("ğŸ“¥ ä¸‹è¼‰æœ€æ–°è³‡æ–™...")
subprocess.run([
    'oci', 'os', 'object', 'get',
    '--namespace', 'nrsdi1rz5vl8',
    '--bucket-name', 'taichung-building-permits',
    '--name', 'data/permits.json',
    '--file', '/tmp/current_permits.json'
], capture_output=True)

with open('/tmp/current_permits.json', 'r') as f:
    data = json.load(f)

# æ‰¾å‡º114å¹´ç©ºç™½è³‡æ–™åºè™Ÿ
empty_sequences = []
for p in data['permits']:
    if p.get('permitYear') == 114:
        seq = p.get('sequenceNumber', 0)
        if not p.get('applicantName', '').strip() or not p.get('siteAddress', '').strip():
            empty_sequences.append(seq)

empty_sequences = sorted(empty_sequences)
print(f"ğŸ” æ‰¾åˆ° {len(empty_sequences)} ç­†ç©ºç™½è³‡æ–™éœ€è¦é‡æ–°çˆ¬å–")

if empty_sequences:
    print(f"ğŸ“‹ åºè™Ÿç¯„åœ: {min(empty_sequences)} - {max(empty_sequences)}")
    print("å‰10ç­†:", empty_sequences[:10])
    
    # å•Ÿå‹•çˆ¬èŸ²
    crawler = OptimizedCrawler()
    crawler.request_delay = 1.0
    crawler.batch_size = 30
    
    print(f"\nğŸš€ é–‹å§‹çˆ¬å–ç©ºç™½è³‡æ–™...")
    print("=" * 70)
    
    # æ‰¹æ¬¡çˆ¬å–
    batch = []
    success_count = 0
    
    for i, seq in enumerate(empty_sequences):
        # çµ„æˆæ­£ç¢ºçš„ index_key
        year = 114
        permit_type = 1
        index_key = f"{year}{permit_type}{seq:05d}00"
        
        print(f"\n[{i+1}/{len(empty_sequences)}] çˆ¬å–åºè™Ÿ {seq:05d} (index_key: {index_key})")
        
        try:
            # ä½¿ç”¨æ­£ç¢ºçš„åƒæ•¸çˆ¬å–
            result = crawler.crawl_single_permit(index_key)
            
            if result and isinstance(result, dict):
                batch.append(result)
                success_count += 1
                print(f"  âœ… {result['permitNumber']}")
                print(f"  ç”³è«‹äºº: {result['applicantName']}")
                print(f"  åœ°å€: {result['siteAddress'][:50]}...")
                
                # æ¯30ç­†ä¸Šå‚³ä¸€æ¬¡
                if len(batch) >= crawler.batch_size:
                    print(f"\nğŸ’¾ ä¸Šå‚³ {len(batch)} ç­†è³‡æ–™...")
                    if crawler.upload_batch_data(batch):
                        print("  âœ… ä¸Šå‚³æˆåŠŸ")
                        batch = []
                    else:
                        print("  âŒ ä¸Šå‚³å¤±æ•—")
                        
            elif result == "NO_DATA":
                print(f"  âš ï¸ ç„¡è³‡æ–™")
            else:
                print(f"  âŒ çˆ¬å–å¤±æ•—")
                
            # å»¶é²é¿å…éå¿«
            time.sleep(crawler.request_delay)
            
        except Exception as e:
            print(f"  âŒ éŒ¯èª¤: {e}")
    
    # ä¸Šå‚³å‰©é¤˜è³‡æ–™
    if batch:
        print(f"\nğŸ’¾ ä¸Šå‚³æœ€å¾Œ {len(batch)} ç­†è³‡æ–™...")
        crawler.upload_batch_data(batch)
    
    # é¡¯ç¤ºçµ±è¨ˆ
    print(f"\nâœ… å®Œæˆï¼æˆåŠŸçˆ¬å– {success_count}/{len(empty_sequences)} ç­†è³‡æ–™")
    crawler.save_progress()
    
else:
    print("âœ… æ²’æœ‰ç©ºç™½è³‡æ–™éœ€è¦çˆ¬å–ï¼")
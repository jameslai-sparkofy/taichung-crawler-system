import io
import json
import logging
import requests
import oci
from bs4 import BeautifulSoup
from datetime import datetime, date
import time
import random
import re

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def handler(ctx, data: io.BytesIO = None):
    """
    OCI Functions è™•ç†å™¨ - å°ä¸­å¸‚å»ºç…§çˆ¬èŸ²
    """
    try:
        logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œå°ä¸­å¸‚å»ºç…§çˆ¬èŸ²")
        
        # åˆå§‹åŒ–çˆ¬èŸ²
        crawler = TaichungBuildingCrawler()
        result = crawler.crawl()
        
        logger.info(f"âœ… çˆ¬èŸ²åŸ·è¡Œå®Œæˆ: {result}")
        
        return {
            "statusCode": 200,
            "body": json.dumps(result, ensure_ascii=False)
        }
        
    except Exception as e:
        logger.error(f"âŒ çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {str(e)}")
        return {
            "statusCode": 500,
            "body": json.dumps({"error": str(e)}, ensure_ascii=False)
        }

class TaichungBuildingCrawler:
    def __init__(self):
        self.base_url = "https://mcgbm.taichung.gov.tw/bupic/pages/queryInfoAction.do"
        self.bucket_name = "taichung-building-permits"
        self.namespace = "nrsdi1rz5vl8"
        self.start_year = 114
        self.crawl_type = 1
        self.delay_ms = 2000
        
        # åˆå§‹åŒ–OCI Object Storageå®¢æˆ¶ç«¯
        self.object_storage_client = oci.object_storage.ObjectStorageClient({})
        
        # çµ±è¨ˆè³‡æ–™
        self.stats = {
            "totalCrawled": 0,
            "newRecords": 0,
            "errorRecords": 0,
            "startTime": datetime.now(),
            "endTime": None
        }

    def generate_index_key(self, year, permit_type, sequence, version=0):
        """ç”ŸæˆINDEX_KEY"""
        return f"{year}{permit_type}{sequence:05d}{version:02d}"

    def parse_index_key(self, index_key):
        """è§£æINDEX_KEY"""
        if len(index_key) != 11:
            return None
        
        return {
            "year": int(index_key[:3]),
            "permitType": int(index_key[3]),
            "sequence": int(index_key[4:9]),
            "version": int(index_key[9:11])
        }

    def fetch_page_with_retry(self, index_key, max_retries=3):
        """ç²å–é é¢å…§å®¹ï¼ˆå«é‡æ–°æ•´ç†æ©Ÿåˆ¶ï¼‰"""
        url = f"{self.base_url}?INDEX_KEY={index_key}"
        
        for attempt in range(max_retries):
            try:
                logger.info(f"ğŸ” çˆ¬å– INDEX_KEY: {index_key} (å˜—è©¦ {attempt + 1}/{max_retries})")
                
                # ç¬¬ä¸€æ¬¡è«‹æ±‚
                response = requests.get(url, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8'
                }, timeout=30)

                if response.status_code == 200:
                    time.sleep(1)  # ç­‰å¾…1ç§’
                    
                    # ç¬¬äºŒæ¬¡è«‹æ±‚ï¼ˆé‡æ–°æ•´ç†ï¼‰
                    response = requests.get(url, headers={
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                        'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8'
                    }, timeout=30)

                    if response.status_code == 200:
                        text = response.text
                        if 'å»ºç¯‰åŸ·ç…§è™Ÿç¢¼' in text or 'â—‹â—‹â—‹ä»£è¡¨éºå¤±å€‹è³‡æ­¡è¿' in text:
                            return text
                        
            except Exception as e:
                logger.error(f"âŒ ç²å–é é¢éŒ¯èª¤ (å˜—è©¦ {attempt + 1}/{max_retries}): {e}")
                
            if attempt < max_retries - 1:
                time.sleep(2)
        
        return None

    def parse_permit_data(self, html_content, index_key):
        """è§£æå»ºç…§è³‡æ–™"""
        try:
            # æª¢æŸ¥æ˜¯å¦åŒ…å«éºå¤±å€‹è³‡
            if 'â—‹â—‹â—‹ä»£è¡¨éºå¤±å€‹è³‡æ­¡è¿' in html_content:
                logger.info(f"â„¹ï¸  INDEX_KEY {index_key}: åŒ…å«éºå¤±å€‹è³‡è¨Šæ¯ï¼Œè·³é")
                return None

            key_info = self.parse_index_key(index_key)
            if not key_info:
                return None

            soup = BeautifulSoup(html_content, 'html.parser')
            
            permit_data = {
                "indexKey": index_key,
                "permitNumber": None,
                "permitYear": key_info["year"],
                "permitType": key_info["permitType"],
                "sequenceNumber": key_info["sequence"],
                "versionNumber": key_info["version"],
                "applicantName": None,
                "designerName": None,
                "designerCompany": None,
                "supervisorName": None,
                "supervisorCompany": None,
                "contractorName": None,
                "contractorCompany": None,
                "engineerName": None,
                "siteAddress": None,
                "siteCity": None,
                "siteZone": None,
                "siteArea": None,
                "landNumber": None,
                "district": None,
                "buildingCount": None,
                "unitCount": None,
                "totalFloorArea": None,
                "issueDate": None,
                "floorInfo": None,
                "crawledAt": datetime.now().isoformat()
            }

            # å»ºç…§è™Ÿç¢¼
            permit_number_match = re.search(r'å»ºé€ åŸ·ç…§è™Ÿç¢¼[ï¼š:\s]*([^\s<\n]+)', html_content)
            if permit_number_match:
                permit_data["permitNumber"] = permit_number_match.group(1).strip()

            # è¡¨æ ¼è³‡æ–™è§£æ
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        label = cells[0].get_text(strip=True)
                        
                        if 'èµ·é€ äºº' in label and len(cells) >= 3:
                            if 'å§“å' in cells[1].get_text(strip=True):
                                permit_data["applicantName"] = cells[2].get_text(strip=True)
                        
                        elif 'è¨­è¨ˆäºº' in label and len(cells) >= 4:
                            if 'å§“å' in cells[1].get_text(strip=True):
                                permit_data["designerName"] = cells[2].get_text(strip=True)
                            if 'äº‹å‹™æ‰€' in cells[3].get_text(strip=True) and len(cells) >= 5:
                                permit_data["designerCompany"] = cells[4].get_text(strip=True)
                        
                        elif 'ç›£é€ äºº' in label and len(cells) >= 4:
                            if 'å§“å' in cells[1].get_text(strip=True):
                                permit_data["supervisorName"] = cells[2].get_text(strip=True)
                            if 'äº‹å‹™æ‰€' in cells[3].get_text(strip=True) and len(cells) >= 5:
                                permit_data["supervisorCompany"] = cells[4].get_text(strip=True)
                        
                        elif 'æ‰¿é€ äºº' in label and len(cells) >= 4:
                            if 'å§“å' in cells[1].get_text(strip=True):
                                permit_data["contractorName"] = cells[2].get_text(strip=True)
                            if len(cells) >= 5:
                                permit_data["contractorCompany"] = cells[4].get_text(strip=True)
                        
                        elif 'å°ˆä»»å·¥ç¨‹äººå“¡' in label:
                            permit_data["engineerName"] = cells[1].get_text(strip=True)
                        
                        elif 'åœ°è™Ÿ' in label:
                            land_text = cells[1].get_text(strip=True)
                            permit_data["siteAddress"] = land_text
                            permit_data["landNumber"] = self.extract_land_number(land_text)
                            permit_data["district"] = self.extract_district(land_text)
                        
                        elif 'åœ°å€' in label:
                            address_text = cells[1].get_text(strip=True)
                            permit_data["siteCity"] = address_text
                            if not permit_data.get("district"):
                                permit_data["district"] = self.extract_district(address_text)
                        
                        elif 'ä½¿ç”¨åˆ†å€' in label:
                            permit_data["siteZone"] = cells[1].get_text(strip=True)
                        
                        elif 'åŸºåœ°é¢ç©' in label and len(cells) >= 4:
                            area_text = cells[3].get_text(strip=True)
                            area_match = re.search(r'([\d.]+)', area_text)
                            if area_match:
                                try:
                                    permit_data["siteArea"] = float(area_match.group(1))
                                except ValueError:
                                    pass
                        
                        elif 'å±¤æ£Ÿæˆ¶æ•¸' in label:
                            floor_info = cells[1].get_text(strip=True)
                            permit_data["floorInfo"] = floor_info
                            building_info = self.extract_building_info(floor_info)
                            permit_data["buildingCount"] = building_info["buildingCount"]
                            permit_data["unitCount"] = building_info["unitCount"]
                        
                        elif 'ç¸½æ¨“åœ°æ¿é¢ç©' in label:
                            area_text = cells[1].get_text(strip=True)
                            area_match = re.search(r'([\d.]+)', area_text)
                            if area_match:
                                try:
                                    permit_data["totalFloorArea"] = float(area_match.group(1))
                                except ValueError:
                                    pass
                        
                        elif 'ç™¼ç…§æ—¥æœŸ' in label:
                            date_text = cells[1].get_text(strip=True)
                            permit_data["issueDate"] = self.parse_issue_date(date_text)

            # æª¢æŸ¥æ˜¯å¦æœ‰å¿…è¦è³‡æ–™
            if not permit_data["permitNumber"]:
                logger.warning(f"âš ï¸  INDEX_KEY {index_key}: æœªæ‰¾åˆ°å»ºç…§è™Ÿç¢¼")
                return None

            return permit_data
            
        except Exception as e:
            logger.error(f"âŒ è§£æå»ºç…§è³‡æ–™éŒ¯èª¤: {e}")
            return None

    def load_existing_data(self):
        """è¼‰å…¥ç¾æœ‰è³‡æ–™"""
        try:
            response = self.object_storage_client.get_object(
                namespace_name=self.namespace,
                bucket_name=self.bucket_name,
                object_name="data/permits.json"
            )
            data = json.loads(response.data.content.decode('utf-8'))
            return data.get("permits", [])
        except Exception as e:
            logger.info(f"è¼‰å…¥ç¾æœ‰è³‡æ–™å¤±æ•— (å¯èƒ½æ˜¯é¦–æ¬¡åŸ·è¡Œ): {e}")
            return []

    def save_data(self, permits):
        """å„²å­˜è³‡æ–™åˆ°Object Storage"""
        try:
            data = {
                "lastUpdate": datetime.now().isoformat(),
                "totalCount": len(permits),
                "permits": permits
            }
            
            content = json.dumps(data, ensure_ascii=False, indent=2)
            
            self.object_storage_client.put_object(
                namespace_name=self.namespace,
                bucket_name=self.bucket_name,
                object_name="data/permits.json",
                put_object_body=content.encode('utf-8'),
                content_type="application/json"
            )
            
            logger.info(f"ğŸ’¾ å·²å„²å­˜ {len(permits)} ç­†å»ºç…§è³‡æ–™")
            
        except Exception as e:
            logger.error(f"å„²å­˜è³‡æ–™éŒ¯èª¤: {e}")

    def save_log(self):
        """å„²å­˜åŸ·è¡Œè¨˜éŒ„"""
        try:
            # è¼‰å…¥ç¾æœ‰è¨˜éŒ„
            try:
                response = self.object_storage_client.get_object(
                    namespace_name=self.namespace,
                    bucket_name=self.bucket_name,
                    object_name="data/crawl-logs.json"
                )
                log_data = json.loads(response.data.content.decode('utf-8'))
                logs = log_data.get("logs", [])
            except:
                logs = []

            log_entry = {
                "date": date.today().isoformat(),
                "startTime": self.stats["startTime"].isoformat(),
                "endTime": self.stats["endTime"].isoformat(),
                "totalCrawled": self.stats["totalCrawled"],
                "newRecords": self.stats["newRecords"],
                "errorRecords": self.stats["errorRecords"],
                "status": "completed" if self.stats["errorRecords"] <= self.stats["newRecords"] else "failed"
            }

            # ç§»é™¤ä»Šæ—¥èˆŠè¨˜éŒ„
            logs = [log for log in logs if log.get("date") != log_entry["date"]]
            logs.insert(0, log_entry)

            # åªä¿ç•™æœ€è¿‘30å¤©è¨˜éŒ„
            logs = logs[:30]

            content = json.dumps({"logs": logs}, ensure_ascii=False, indent=2)
            
            self.object_storage_client.put_object(
                namespace_name=self.namespace,
                bucket_name=self.bucket_name,
                object_name="data/crawl-logs.json",
                put_object_body=content.encode('utf-8'),
                content_type="application/json"
            )
            
            logger.info("ğŸ“‹ å·²å„²å­˜åŸ·è¡Œè¨˜éŒ„")
            
        except Exception as e:
            logger.error(f"å„²å­˜è¨˜éŒ„éŒ¯èª¤: {e}")

    def get_max_sequence_number(self, permits, year, permit_type):
        """å–å¾—æœ€å¤§åºè™Ÿ"""
        filtered = [p for p in permits if p.get("permitYear") == year and p.get("permitType") == permit_type]
        if not filtered:
            return 0
        return max([p.get("sequenceNumber", 0) for p in filtered])

    def crawl_single_permit(self, index_key):
        """çˆ¬å–å–®ä¸€å»ºç…§"""
        try:
            html_content = self.fetch_page_with_retry(index_key)
            if not html_content:
                logger.error(f"âŒ ç„¡æ³•ç²å–é é¢: {index_key}")
                self.stats["errorRecords"] += 1
                return None

            permit_data = self.parse_permit_data(html_content, index_key)
            if not permit_data:
                return None

            logger.info(f"âœ… æˆåŠŸè§£æå»ºç…§: {permit_data['permitNumber']}")
            self.stats["newRecords"] += 1
            self.stats["totalCrawled"] += 1
            
            return permit_data
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–å»ºç…§éŒ¯èª¤: {e}")
            self.stats["errorRecords"] += 1
            return None

    def crawl(self):
        """ä¸»è¦çˆ¬èŸ²åŸ·è¡Œ"""
        logger.info(f"ğŸ“… åŸ·è¡Œæ™‚é–“: {self.stats['startTime'].isoformat()}")

        try:
            # è¼‰å…¥ç¾æœ‰è³‡æ–™
            existing_permits = self.load_existing_data()
            logger.info(f"ğŸ“Š ç¾æœ‰è³‡æ–™: {len(existing_permits)} ç­†")

            # ç²å–æœ€å¤§åºè™Ÿ
            max_sequence = self.get_max_sequence_number(existing_permits, self.start_year, self.crawl_type)
            current_sequence = max(max_sequence, 1)
            consecutive_failures = 0
            max_consecutive_failures = 50

            logger.info(f"ğŸ”¢ å¾åºè™Ÿ {current_sequence} é–‹å§‹çˆ¬å–")

            new_permits = []

            # é™åˆ¶FunctionsåŸ·è¡Œæ™‚é–“ï¼Œæœ€å¤šçˆ¬å–50ç­†
            while consecutive_failures < max_consecutive_failures and len(new_permits) < 50:
                index_key = self.generate_index_key(self.start_year, self.crawl_type, current_sequence)
                permit_data = self.crawl_single_permit(index_key)

                if permit_data:
                    # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    exists = any(p.get("indexKey") == index_key for p in existing_permits)
                    if not exists:
                        new_permits.append(permit_data)
                        consecutive_failures = 0
                else:
                    consecutive_failures += 1

                current_sequence += 1
                
                # å»¶é²é¿å…éåº¦è«‹æ±‚
                time.sleep(self.delay_ms / 1000)

            # åˆä½µæ–°èˆŠè³‡æ–™
            all_permits = existing_permits + new_permits
            
            # ä¾åºè™Ÿæ’åº
            all_permits.sort(key=lambda x: (
                -x.get("permitYear", 0),
                x.get("permitType", 0),
                -x.get("sequenceNumber", 0)
            ))

            # å„²å­˜è³‡æ–™
            self.save_data(all_permits)

            self.stats["endTime"] = datetime.now()
            self.save_log()

            result = {
                "success": True,
                "totalCrawled": self.stats["totalCrawled"],
                "newRecords": self.stats["newRecords"],
                "errorRecords": self.stats["errorRecords"],
                "executionTime": (self.stats["endTime"] - self.stats["startTime"]).total_seconds()
            }

            logger.info(f"âœ… çˆ¬èŸ²ä»»å‹™å®Œæˆ: ç¸½è¨ˆ {self.stats['totalCrawled']} ç­†ï¼Œæ–°å¢ {self.stats['newRecords']} ç­†ï¼ŒéŒ¯èª¤ {self.stats['errorRecords']} ç­†")
            
            return result

        except Exception as e:
            self.stats["endTime"] = datetime.now()
            self.save_log()
            logger.error(f"âŒ çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {e}")
            
            return {
                "success": False,
                "error": str(e),
                "totalCrawled": self.stats["totalCrawled"],
                "newRecords": self.stats["newRecords"],
                "errorRecords": self.stats["errorRecords"]
            }
    
    def extract_land_number(self, text):
        """å¾æ–‡å­—ä¸­æå–åœ°è™Ÿ"""
        if not text:
            return None
        # åŒ¹é…å®Œæ•´åœ°è™Ÿæ ¼å¼
        match = re.search(r'(è‡ºä¸­å¸‚[^å€]+å€[^æ®µ]+æ®µ[\d-]+åœ°è™Ÿ)', text)
        if match:
            return match.group(1)
        # åŒ¹é…ç°¡åŒ–åœ°è™Ÿæ ¼å¼
        match = re.search(r'([^å¸‚å€]+æ®µ[\d-]+åœ°è™Ÿ)', text)
        if match:
            return match.group(1)
        return text

    def extract_district(self, text):
        """å¾æ–‡å­—ä¸­æå–è¡Œæ”¿å€"""
        if not text:
            return None
        # å…ˆå˜—è©¦å¾å®Œæ•´æ ¼å¼ä¸­æå–
        match = re.search(r'è‡ºä¸­å¸‚([^å€]+å€)', text)
        if match:
            return match.group(1)
        # åŒ¹é…å·²çŸ¥çš„è¡Œæ”¿å€
        districts = [
            'è¥¿å±¯å€', 'å—å±¯å€', 'åŒ—å±¯å€', 'è¥¿å€', 'å—å€', 'åŒ—å€', 'æ±å€', 'ä¸­å€',
            'å¤ªå¹³å€', 'å¤§é‡Œå€', 'çƒæ—¥å€', 'è±åŸå€', 'éœ§å³°å€', 'å¤§é›…å€', 'æ½­å­å€',
            'åé‡Œå€', 'ç¥å²¡å€', 'æ¢§æ£²å€', 'æ¸…æ°´å€', 'å¤§ç”²å€', 'å¤–åŸ”å€', 'å¤§å®‰å€',
            'æ²™é¹¿å€', 'é¾äº•å€', 'å¤§è‚šå€', 'æ–°ç¤¾å€', 'çŸ³å²¡å€', 'æ±å‹¢å€', 'å’Œå¹³å€'
        ]
        for district in districts:
            if district in text:
                return district
        return None

    def extract_building_info(self, text):
        """å¾å±¤æ£Ÿæˆ¶æ•¸æ–‡å­—ä¸­æå–æ£Ÿæ•¸å’Œæˆ¶æ•¸"""
        if not text:
            return {'buildingCount': None, 'unitCount': None}
        
        building_match = re.search(r'(\d+)æ£Ÿ', text)
        unit_match = re.search(r'(\d+)æˆ¶', text)
        
        return {
            'buildingCount': int(building_match.group(1)) if building_match else None,
            'unitCount': int(unit_match.group(1)) if unit_match else None
        }
    
    def parse_issue_date(self, text):
        """è§£æç™¼ç…§æ—¥æœŸ"""
        if not text:
            return None
        # å˜—è©¦è§£ææ°‘åœ‹å¹´æ ¼å¼
        date_match = re.search(r'(\d+)å¹´(\d+)æœˆ(\d+)æ—¥', text)
        if date_match:
            try:
                year = int(date_match.group(1)) + 1911  # æ°‘åœ‹å¹´è½‰è¥¿å…ƒå¹´
                month = int(date_match.group(2))
                day = int(date_match.group(3))
                return f"{year:04d}-{month:02d}-{day:02d}"
            except:
                return text
        return text
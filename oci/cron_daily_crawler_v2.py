#!/usr/bin/env python3
"""
æ¯æ—¥è‡ªå‹•çˆ¬èŸ² V2 - æŒçºŒçˆ¬å–ç›´åˆ°é‡åˆ°ç©ºç™½
"""

import sys
import os
import json
import time
from datetime import datetime

# åˆ‡æ›åˆ°æ­£ç¢ºçš„å·¥ä½œç›®éŒ„
os.chdir('/mnt/c/claude code/å»ºç…§çˆ¬èŸ²/oci')

# æ·»åŠ è·¯å¾‘
sys.path.append('/mnt/c/claude code/å»ºç…§çˆ¬èŸ²/oci')

# è¼‰å…¥çˆ¬èŸ²
exec(open('optimized-crawler-stable.py').read().split('if __name__ == "__main__":')[0])

# é€²åº¦æª”æ¡ˆ
PROGRESS_FILE = 'cron_progress.json'

def load_progress():
    """è¼‰å…¥çˆ¬èŸ²é€²åº¦"""
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, 'r') as f:
            return json.load(f)
    else:
        # é è¨­å¾114å¹´åºè™Ÿ1099é–‹å§‹ï¼ˆå› ç‚º1098å·²ç¶“çˆ¬éäº†ï¼‰
        return {
            "year": 114,
            "currentSequence": 1099,
            "lastCrawledAt": None,
            "consecutiveEmpty": 0
        }

def save_progress(progress):
    """å„²å­˜çˆ¬èŸ²é€²åº¦"""
    with open(PROGRESS_FILE, 'w') as f:
        json.dump(progress, f, ensure_ascii=False, indent=2)

def daily_crawl():
    """æ¯æ—¥çˆ¬èŸ²ä»»å‹™ - æŒçºŒçˆ¬å–ç›´åˆ°é‡åˆ°ç©ºç™½"""
    
    print(f"ğŸ• æ¯æ—¥çˆ¬èŸ² V2 é–‹å§‹: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # è¼‰å…¥é€²åº¦
    progress = load_progress()
    year = progress['year']
    current_seq = progress['currentSequence']
    consecutive_empty = progress['consecutiveEmpty']
    
    print(f"ğŸ“Š ç•¶å‰é€²åº¦: {year}å¹´ åºè™Ÿ{current_seq}")
    print(f"   é€£çºŒç©ºç™½: {consecutive_empty}æ¬¡")
    
    # å‰µå»ºçˆ¬èŸ²å¯¦ä¾‹
    crawler = OptimizedCrawler()
    crawler.request_delay = 0.8
    
    # è¨˜éŒ„æœ¬æ¬¡åŸ·è¡Œçš„çµ±è¨ˆ
    crawled_count = 0
    success_count = 0
    empty_count = 0
    error_count = 0
    batch_data = []
    
    # æŒçºŒçˆ¬å–ç›´åˆ°é‡åˆ°ç©ºç™½
    while True:
        index_key = f'{year}10{current_seq:04d}00'
        print(f'\nğŸ” [{crawled_count + 1}] çˆ¬å–: {index_key}')
        
        result = crawler.crawl_single_permit(index_key)
        crawled_count += 1
        
        if result and result != 'NO_DATA':
            # çˆ¬å–æˆåŠŸ
            print(f'âœ… çˆ¬å–æˆåŠŸ!')
            print(f'   å»ºç…§è™Ÿç¢¼: {result.get("permitNumber")}')
            print(f'   ç”³è«‹äºº: {result.get("applicantName")}')
            print(f'   ç¸½æ¨“åœ°æ¿é¢ç©: {result.get("totalFloorArea")}')
            
            success_count += 1
            batch_data.append(result)
            
            # æ›´æ–°é€²åº¦
            progress['currentSequence'] = current_seq + 1
            progress['lastCrawledAt'] = datetime.now().isoformat()
            progress['consecutiveEmpty'] = 0  # é‡ç½®é€£çºŒç©ºç™½è¨ˆæ•¸
            consecutive_empty = 0
            
            # æ¯20ç­†ä¸Šå‚³ä¸€æ¬¡
            if len(batch_data) >= 20:
                print(f'\nğŸ’¾ æ‰¹æ¬¡ä¸Šå‚³ {len(batch_data)} ç­†è³‡æ–™...')
                success = crawler.upload_batch_data(batch_data)
                if success:
                    print('âœ… æ‰¹æ¬¡ä¸Šå‚³æˆåŠŸ!')
                else:
                    print('âŒ æ‰¹æ¬¡ä¸Šå‚³å¤±æ•—!')
                batch_data = []
            
            # ç¹¼çºŒä¸‹ä¸€å€‹åºè™Ÿ
            current_seq += 1
            
        elif result == 'NO_DATA':
            # æ­¤åºè™Ÿç„¡è³‡æ–™ï¼ˆå»ºç…§å°šæœªç™¼å‡ºï¼‰
            print('âš ï¸ æ­¤åºè™ŸæŸ¥ç„¡è³‡æ–™ï¼ˆå»ºç…§å°šæœªç™¼å‡ºï¼‰')
            empty_count += 1
            consecutive_empty += 1
            progress['consecutiveEmpty'] = consecutive_empty
            
            # æ›´æ–°é€²åº¦
            progress['currentSequence'] = current_seq + 1
            progress['lastCrawledAt'] = datetime.now().isoformat()
            
            if consecutive_empty >= 3:
                # é€£çºŒ3å€‹åºè™Ÿéƒ½ç„¡è³‡æ–™ï¼Œåœæ­¢çˆ¬å–
                print(f'\nğŸ é€£çºŒ{consecutive_empty}å€‹åºè™Ÿç„¡è³‡æ–™ï¼Œåœæ­¢çˆ¬å–')
                print(f'ğŸ“Œ æœ€å¾Œæœ‰æ•ˆåºè™Ÿ: {current_seq - consecutive_empty}')
                break
            else:
                # ç¹¼çºŒå˜—è©¦ä¸‹ä¸€å€‹åºè™Ÿ
                current_seq += 1
                
        else:
            # çˆ¬å–å¤±æ•—ï¼ˆå¯èƒ½æ˜¯ç¶²è·¯å•é¡Œæˆ–æ ¼å¼éŒ¯èª¤ï¼‰
            print(f'âŒ çˆ¬å–å¤±æ•—')
            print(f'   è¿”å›å€¼: {result}')
            print(f'   å¯èƒ½åŸå› : ç¶²è·¯å•é¡Œã€ç¶²ç«™æš«æ™‚ç„¡æ³•è¨ªå•ã€æˆ–åºè™Ÿæ ¼å¼éŒ¯èª¤')
            error_count += 1
            
            # å˜—è©¦è·³éé€™å€‹åºè™Ÿ
            progress['currentSequence'] = current_seq + 1
            progress['lastCrawledAt'] = datetime.now().isoformat()
            current_seq += 1
            
            # å¦‚æœéŒ¯èª¤å¤ªå¤šï¼Œåœæ­¢
            if error_count >= 5:
                print('\nâŒ éŒ¯èª¤æ¬¡æ•¸éå¤šï¼Œåœæ­¢çˆ¬å–')
                break
        
        # å„²å­˜é€²åº¦
        save_progress(progress)
        
        # å»¶é²é¿å…è¢«å°é–
        time.sleep(crawler.request_delay)
        
        # å¦‚æœå·²çˆ¬å–è¶…é50ç­†ï¼Œå…ˆåœæ­¢ï¼ˆé¿å…åŸ·è¡Œå¤ªä¹…ï¼‰
        if crawled_count >= 50:
            print('\nâ¸ï¸ å·²çˆ¬å–50ç­†ï¼Œæš«åœåŸ·è¡Œ')
            break
    
    # ä¸Šå‚³å‰©é¤˜çš„è³‡æ–™
    if batch_data:
        print(f'\nğŸ’¾ ä¸Šå‚³å‰©é¤˜ {len(batch_data)} ç­†è³‡æ–™...')
        success = crawler.upload_batch_data(batch_data)
        if success:
            print('âœ… ä¸Šå‚³æˆåŠŸ!')
        else:
            print('âŒ ä¸Šå‚³å¤±æ•—!')
    
    # é¡¯ç¤ºçµ±è¨ˆ
    print(f"\nğŸ“Š æœ¬æ¬¡åŸ·è¡Œçµ±è¨ˆ:")
    print(f"   ç¸½çˆ¬å–: {crawled_count} ç­†")
    print(f"   æˆåŠŸ: {success_count} ç­†")
    print(f"   ç©ºç™½: {empty_count} ç­†")
    print(f"   éŒ¯èª¤: {error_count} ç­†")
    print(f"   æœ€æ–°åºè™Ÿ: {progress['currentSequence']}")
    
    print(f"\nğŸ• æ¯æ—¥çˆ¬èŸ² V2 çµæŸ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    daily_crawl()
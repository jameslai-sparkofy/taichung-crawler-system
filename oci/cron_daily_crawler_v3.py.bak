#!/usr/bin/env python3
"""
æ¯æ—¥è‡ªå‹•çˆ¬èŸ² V3 - å¾è³‡æ–™åº«æœ€æ–°åºè™Ÿé–‹å§‹
"""

import sys
import os
import json
import time
import subprocess
from datetime import datetime

# åˆ‡æ›åˆ°æ­£ç¢ºçš„å·¥ä½œç›®éŒ„
os.chdir('/mnt/c/claude code/å»ºç…§çˆ¬èŸ²/oci')

# æ·»åŠ è·¯å¾‘
sys.path.append('/mnt/c/claude code/å»ºç…§çˆ¬èŸ²/oci')

# è¼‰å…¥çˆ¬èŸ²
exec(open('optimized-crawler-stable.py').read().split('if __name__ == "__main__":')[0])

def get_latest_sequence():
    """å¾OCIç²å–æœ€æ–°åºè™Ÿ"""
    try:
        # ä¸‹è¼‰æœ€æ–°è³‡æ–™
        cmd = [
            "/home/laija/bin/oci", "os", "object", "get",
            "--namespace", "nrsdi1rz5vl8",
            "--bucket-name", "taichung-building-permits",
            "--name", "data/permits.json",
            "--file", "/tmp/current_permits.json"
        ]
        subprocess.run(cmd, capture_output=True)
        
        # è®€å–ä¸¦åˆ†æ
        with open('/tmp/current_permits.json', 'r') as f:
            data = json.load(f)
        
        # æ‰¾å‡º114å¹´çš„æœ€å¤§åºè™Ÿ
        seq_114 = [p['sequenceNumber'] for p in data['permits'] if p.get('permitYear') == 114]
        if seq_114:
            latest_seq = max(seq_114)
            print(f"ğŸ“Š è³‡æ–™åº«æœ€æ–°åºè™Ÿ: {latest_seq}")
            return latest_seq + 1  # å¾ä¸‹ä¸€å€‹é–‹å§‹
        else:
            print("ğŸ“Š è³‡æ–™åº«ç„¡114å¹´è³‡æ–™ï¼Œå¾1é–‹å§‹")
            return 1
            
    except Exception as e:
        print(f"âŒ ç„¡æ³•å–å¾—æœ€æ–°åºè™Ÿ: {e}")
        print("ğŸ“Š ä½¿ç”¨é è¨­åºè™Ÿ: 1100")
        return 1100

def daily_crawl():
    """æ¯æ—¥çˆ¬èŸ²ä»»å‹™ - å¾è³‡æ–™åº«æœ€æ–°åºè™Ÿé–‹å§‹"""
    
    print(f"ğŸ• æ¯æ—¥çˆ¬èŸ² V3 é–‹å§‹: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # ç²å–èµ·å§‹åºè™Ÿ
    year = 114
    current_seq = get_latest_sequence()
    
    print(f"ğŸ“Š é–‹å§‹çˆ¬å–: {year}å¹´ åºè™Ÿ{current_seq}")
    
    # å‰µå»ºçˆ¬èŸ²å¯¦ä¾‹
    crawler = OptimizedCrawler()
    crawler.request_delay = 0.8
    
    # è¨˜éŒ„æœ¬æ¬¡åŸ·è¡Œçš„çµ±è¨ˆ
    crawled_count = 0
    success_count = 0
    empty_count = 0
    error_count = 0
    batch_data = []
    consecutive_empty = 0
    
    # æŒçºŒçˆ¬å–ç›´åˆ°é‡åˆ°ç©ºç™½
    while True:
        index_key = f'{year}1{current_seq:05d}00'  # æ³¨æ„ï¼šé¡å‹æ˜¯1ä¸æ˜¯10
        print(f'\nğŸ” [{crawled_count + 1}] çˆ¬å–: {index_key}')
        
        result = crawler.crawl_single_permit(index_key)
        crawled_count += 1
        
        if result and result != 'NO_DATA':
            # çˆ¬å–æˆåŠŸ
            print(f'âœ… çˆ¬å–æˆåŠŸ!')
            print(f'   å»ºç…§è™Ÿç¢¼: {result.get("permitNumber")}')
            print(f'   ç”³è«‹äºº: {result.get("applicantName")}')
            print(f'   ç¸½æ¨“åœ°æ¿é¢ç©: {result.get("totalFloorArea")}')
            
            success_count += 1
            batch_data.append(result)
            consecutive_empty = 0  # é‡ç½®é€£çºŒç©ºç™½è¨ˆæ•¸
            
            # æ¯20ç­†ä¸Šå‚³ä¸€æ¬¡
            if len(batch_data) >= 20:
                print(f'\nğŸ’¾ æ‰¹æ¬¡ä¸Šå‚³ {len(batch_data)} ç­†è³‡æ–™...')
                success = crawler.upload_batch_data(batch_data)
                if success:
                    print('âœ… æ‰¹æ¬¡ä¸Šå‚³æˆåŠŸ!')
                else:
                    print('âŒ æ‰¹æ¬¡ä¸Šå‚³å¤±æ•—!')
                batch_data = []
            
            # ç¹¼çºŒä¸‹ä¸€å€‹åºè™Ÿ
            current_seq += 1
            
        elif result == 'NO_DATA':
            # æ­¤åºè™Ÿç„¡è³‡æ–™ï¼ˆå»ºç…§å°šæœªç™¼å‡ºï¼‰
            print('âš ï¸ æ­¤åºè™ŸæŸ¥ç„¡è³‡æ–™ï¼ˆå»ºç…§å°šæœªç™¼å‡ºï¼‰')
            empty_count += 1
            consecutive_empty += 1
            
            if consecutive_empty >= 3:
                # é€£çºŒ3å€‹åºè™Ÿéƒ½ç„¡è³‡æ–™ï¼Œåœæ­¢çˆ¬å–
                print(f'\nğŸ é€£çºŒ{consecutive_empty}å€‹åºè™Ÿç„¡è³‡æ–™ï¼Œåœæ­¢çˆ¬å–')
                print(f'ğŸ“Œ æœ€å¾Œæœ‰æ•ˆåºè™Ÿ: {current_seq - consecutive_empty}')
                break
            else:
                # ç¹¼çºŒå˜—è©¦ä¸‹ä¸€å€‹åºè™Ÿ
                current_seq += 1
                
        else:
            # çˆ¬å–å¤±æ•—ï¼ˆå¯èƒ½æ˜¯ç¶²è·¯å•é¡Œæˆ–æ ¼å¼éŒ¯èª¤ï¼‰
            print(f'âŒ çˆ¬å–å¤±æ•—')
            print(f'   è¿”å›å€¼: {result}')
            print(f'   å¯èƒ½åŸå› : ç¶²è·¯å•é¡Œã€ç¶²ç«™æš«æ™‚ç„¡æ³•è¨ªå•ã€æˆ–åºè™Ÿæ ¼å¼éŒ¯èª¤')
            error_count += 1
            
            # å˜—è©¦è·³éé€™å€‹åºè™Ÿ
            current_seq += 1
            
            # å¦‚æœéŒ¯èª¤å¤ªå¤šï¼Œåœæ­¢
            if error_count >= 5:
                print('\nâŒ éŒ¯èª¤æ¬¡æ•¸éå¤šï¼Œåœæ­¢çˆ¬å–')
                break
        
        # å»¶é²é¿å…è¢«å°é–
        time.sleep(crawler.request_delay)
        
        # å¦‚æœå·²çˆ¬å–è¶…é50ç­†ï¼Œå…ˆåœæ­¢ï¼ˆé¿å…åŸ·è¡Œå¤ªä¹…ï¼‰
        if crawled_count >= 50:
            print('\nâ¸ï¸ å·²çˆ¬å–50ç­†ï¼Œæš«åœåŸ·è¡Œ')
            break
    
    # ä¸Šå‚³å‰©é¤˜çš„è³‡æ–™
    if batch_data:
        print(f'\nğŸ’¾ ä¸Šå‚³å‰©é¤˜ {len(batch_data)} ç­†è³‡æ–™...')
        success = crawler.upload_batch_data(batch_data)
        if success:
            print('âœ… ä¸Šå‚³æˆåŠŸ!')
        else:
            print('âŒ ä¸Šå‚³å¤±æ•—!')
    
    # æ›´æ–°åŸ·è¡Œè¨˜éŒ„åˆ°OCI
    update_crawl_logs(success_count, error_count, empty_count, current_seq - 1)
    
    # é¡¯ç¤ºçµ±è¨ˆ
    print(f"\nğŸ“Š æœ¬æ¬¡åŸ·è¡Œçµ±è¨ˆ:")
    print(f"   ç¸½çˆ¬å–: {crawled_count} ç­†")
    print(f"   æˆåŠŸ: {success_count} ç­†")
    print(f"   ç©ºç™½: {empty_count} ç­†")
    print(f"   éŒ¯èª¤: {error_count} ç­†")
    print(f"   æœ€æ–°åºè™Ÿ: {current_seq - 1}")
    
    print(f"\nğŸ• æ¯æ—¥çˆ¬èŸ² V3 çµæŸ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

def update_crawl_logs(success_count, error_count, empty_count, last_sequence):
    """æ›´æ–°åŸ·è¡Œè¨˜éŒ„åˆ°OCI"""
    try:
        # ä¸‹è¼‰ç¾æœ‰è¨˜éŒ„
        cmd = [
            "/home/laija/bin/oci", "os", "object", "get",
            "--namespace", "nrsdi1rz5vl8",
            "--bucket-name", "taichung-building-permits",
            "--name", "data/crawl-logs.json",
            "--file", "/tmp/crawl_logs.json"
        ]
        subprocess.run(cmd, capture_output=True)
        
        with open('/tmp/crawl_logs.json', 'r') as f:
            log_data = json.load(f)
        
        # å»ºç«‹æ–°è¨˜éŒ„
        now = datetime.now()
        new_log = {
            "date": now.strftime("%Y-%m-%d"),
            "startTime": now.replace(second=0, microsecond=0).isoformat(),
            "endTime": now.isoformat(),
            "totalCrawled": success_count + error_count + empty_count,
            "newRecords": success_count,
            "errorRecords": error_count,
            "status": "completed" if success_count > 0 else "partial",
            "message": f"å®šæ™‚çˆ¬èŸ²V3åŸ·è¡Œ - çˆ¬å–è‡³åºè™Ÿ{last_sequence}ï¼ŒæˆåŠŸ{success_count}ç­†"
        }
        
        # æ’å…¥åˆ°æœ€å‰é¢
        log_data['logs'].insert(0, new_log)
        
        # åªä¿ç•™æœ€è¿‘50ç­†
        log_data['logs'] = log_data['logs'][:50]
        
        # å„²å­˜ä¸¦ä¸Šå‚³
        with open('/tmp/crawl_logs_new.json', 'w') as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)
        
        cmd = [
            "/home/laija/bin/oci", "os", "object", "put",
            "--namespace", "nrsdi1rz5vl8",
            "--bucket-name", "taichung-building-permits",
            "--name", "data/crawl-logs.json",
            "--file", "/tmp/crawl_logs_new.json",
            "--content-type", "application/json",
            "--force"
        ]
        subprocess.run(cmd, capture_output=True)
        print("âœ… åŸ·è¡Œè¨˜éŒ„å·²æ›´æ–°åˆ°OCI")
        
    except Exception as e:
        print(f"âŒ æ›´æ–°åŸ·è¡Œè¨˜éŒ„å¤±æ•—: {e}")

if __name__ == "__main__":
    daily_crawl()
#!/usr/bin/env python3
"""
æ ¹æ“šä¿®å¾©æ¸…å–®ä¿®å¾©ç©ºç™½æ¬„ä½
ä½¿ç”¨ fix_empty_[year].txt æ¸…å–®é€²è¡Œä¿®å¾©
"""

import sys
import json
import time
from datetime import datetime

# è¼‰å…¥çˆ¬èŸ²
exec(open('optimized-crawler-stable.py').read().split('if __name__ == "__main__":')[0])

def load_fix_list(year):
    """è¼‰å…¥æŒ‡å®šå¹´ä»½çš„ä¿®å¾©æ¸…å–®"""
    filename = f"fix_empty_{year}.txt"
    sequences = []
    
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    parts = line.split()
                    if len(parts) >= 2:
                        seq_year, seq_num = parts[0], int(parts[1])
                        if seq_year == str(year):
                            sequences.append(seq_num)
        
        return sorted(sequences)
    except FileNotFoundError:
        print(f"âŒ ä¿®å¾©æ¸…å–®æª”æ¡ˆä¸å­˜åœ¨: {filename}")
        return []

def main():
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ–¹æ³•: python3 fix_empty_fields_by_list.py <å¹´ä»½>")
        print("ä¾‹å¦‚: python3 fix_empty_fields_by_list.py 112")
        sys.exit(1)
    
    year = int(sys.argv[1])
    print(f"ğŸ”§ é–‹å§‹ä¿®å¾© {year}å¹´ ç©ºç™½æ¬„ä½è³‡æ–™")
    
    # è¼‰å…¥ä¿®å¾©æ¸…å–®
    sequences = load_fix_list(year)
    if not sequences:
        print(f"âŒ æ²’æœ‰æ‰¾åˆ° {year}å¹´ çš„ä¿®å¾©æ¸…å–®")
        return
    
    print(f"ğŸ“‹ è¼‰å…¥ä¿®å¾©æ¸…å–®: {len(sequences)} ç­†")
    print(f"ğŸ“Š åºè™Ÿç¯„åœ: {min(sequences)} - {max(sequences)}")
    
    # åˆå§‹åŒ–çˆ¬èŸ²
    crawler = OptimizedCrawler()
    crawler.request_delay = 0.8
    crawler.batch_size = 30
    
    print(f"\nğŸš€ é–‹å§‹ä¿®å¾©ç©ºç™½æ¬„ä½...")
    print("=" * 70)
    
    success_count = 0
    failed_count = 0
    
    for i, seq in enumerate(sequences):
        try:
            print(f"ğŸ” [{seq:05d}] ä¿®å¾©ä¸­...", end=" ")
            
            # çˆ¬å–å–®ç­†è³‡æ–™
            index_key = f"{year}1{seq:05d}00"
            permit_data = crawler.crawl_single_permit(index_key)
            
            if permit_data:
                print("âœ…")
                success_count += 1
            else:
                print("âŒ ç„¡è³‡æ–™")
                failed_count += 1
            
            # æ¯30ç­†ä¸Šå‚³ä¸€æ¬¡
            if (i + 1) % crawler.batch_size == 0 and len(crawler.results) > 0:
                print(f"\nğŸ’¾ æ‰¹æ¬¡ä¸Šå‚³ ({len(crawler.results)} ç­†)...")
                try:
                    crawler.upload_batch_data(crawler.results)
                    crawler.results = []  # æ¸…ç©ºå·²ä¸Šå‚³çš„è³‡æ–™
                    print("âœ… ä¸Šå‚³æˆåŠŸ")
                    print(f"ğŸ“Š [{i+1:4d}/{len(sequences)}] æˆåŠŸ:{success_count} å¤±æ•—:{failed_count}")
                    print("-" * 50)
                except Exception as e:
                    print(f"âŒ ä¸Šå‚³å¤±æ•—: {e}")
            
            # å»¶é²é¿å…è¢«å°é–
            if crawler.request_delay > 0:
                time.sleep(crawler.request_delay)
                
        except Exception as e:
            print(f"âŒ éŒ¯èª¤: {e}")
            failed_count += 1
    
    # ä¸Šå‚³å‰©é¤˜è³‡æ–™
    if len(crawler.results) > 0:
        print(f"\nğŸ’¾ ä¸Šå‚³å‰©é¤˜è³‡æ–™ ({len(crawler.results)} ç­†)...")
        try:
            crawler.upload_batch_data(crawler.results)
            print("âœ… ä¸Šå‚³æˆåŠŸ")
        except Exception as e:
            print(f"âŒ ä¸Šå‚³å¤±æ•—: {e}")
    
    # æœ€çµ‚çµ±è¨ˆ
    print("\n" + "=" * 70)
    print("ğŸ ä¿®å¾©å®Œæˆï¼")
    print(f"ğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
    print(f"   ç¸½è¨ˆå˜—è©¦: {len(sequences)}")
    print(f"   æˆåŠŸä¿®å¾©: {success_count}")
    print(f"   å¤±æ•—: {failed_count}")
    print(f"   æˆåŠŸç‡: {success_count/len(sequences)*100:.1f}%")

if __name__ == "__main__":
    main()
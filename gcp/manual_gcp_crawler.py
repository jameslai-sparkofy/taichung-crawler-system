#!/usr/bin/env python3
"""
Google Cloud æ‰‹å‹•çˆ¬èŸ²ç¨‹å¼
å¯åœ¨æœ¬åœ°åŸ·è¡Œï¼Œè³‡æ–™æœƒå„²å­˜åˆ° Google Cloud Storage
"""
import os
import json
import time
import requests
from datetime import datetime
from bs4 import BeautifulSoup
import subprocess
import re

# è¨­å®š
BUCKET_NAME = "taichung-crawler-permits"
LOCAL_DATA_FILE = "permits.json"

def check_taiwan_ip():
    """æª¢æŸ¥æ˜¯å¦ç‚ºå°ç£ IP"""
    try:
        response = requests.get("https://ipapi.co/json/", timeout=5)
        data = response.json()
        country = data.get('country', 'Unknown')
        ip = data.get('ip', 'Unknown')
        print(f"ğŸ“ ç›®å‰ IP: {ip} ({country})")
        return country == 'TW'
    except:
        return False

def upload_to_gcs(file_path, gcs_path):
    """ä¸Šå‚³æª”æ¡ˆåˆ° Google Cloud Storage"""
    try:
        cmd = f"gsutil cp {file_path} gs://{BUCKET_NAME}/{gcs_path}"
        subprocess.run(cmd, shell=True, check=True)
        print(f"âœ… å·²ä¸Šå‚³åˆ° GCS: {gcs_path}")
        return True
    except Exception as e:
        print(f"âŒ ä¸Šå‚³å¤±æ•—: {e}")
        print("   è«‹ç¢ºèªå·²å®‰è£ gsutil ä¸¦ç™»å…¥ Google Cloud")
        return False

def download_from_gcs(gcs_path, local_path):
    """å¾ Google Cloud Storage ä¸‹è¼‰æª”æ¡ˆ"""
    try:
        cmd = f"gsutil cp gs://{BUCKET_NAME}/{gcs_path} {local_path}"
        subprocess.run(cmd, shell=True, check=True)
        return True
    except:
        print(f"âš ï¸  ç„¡æ³•ä¸‹è¼‰ {gcs_path}ï¼Œå°‡ä½¿ç”¨æ–°æª”æ¡ˆ")
        return False

def crawl_single_permit(year, seq):
    """çˆ¬å–å–®ç­†å»ºç…§è³‡æ–™"""
    index_key = f"{year}10{seq:05d}00"
    url = f"https://mcgbm.taichung.gov.tw/bupic/pages/queryInfoAction.do?INDEX_KEY={index_key}"
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.encoding = 'utf-8'
        
        if response.status_code != 200:
            return None
            
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºç©ºç™½é é¢
        if "æŸ¥ç„¡æ­¤æ¡ˆä»¶ç·¨è™Ÿè³‡æ–™" in response.text or len(response.text) < 1000:
            return "BLANK"
        
        # è§£æå»ºç…§è³‡æ–™
        permit_data = {
            'indexKey': index_key,
            'year': year,
            'sequence': seq,
            'crawlTime': datetime.now().isoformat()
        }
        
        # è§£æå„æ¬„ä½
        fields = soup.find_all('td', class_='tit_intd2 w15')
        for field in fields:
            label = field.text.strip()
            value_elem = field.find_next_sibling('td')
            if value_elem:
                value = value_elem.text.strip()
                
                if 'å»ºç…§è™Ÿç¢¼' in label:
                    permit_data['permitNumber'] = value
                elif 'ç”³è«‹äºº' in label:
                    permit_data['applicant'] = value
                elif 'åœ°å€' in label:
                    permit_data['address'] = value
                elif 'å»ºç¯‰ç‰©æ¦‚è¦' in label:
                    # è§£ææ¨“å±¤ã€æ£Ÿæ•¸ã€æˆ¶æ•¸
                    floor_info = value
                    permit_data['buildingSummary'] = floor_info
                    
                    floor_match = re.search(r'åœ°ä¸Š(\d+)å±¤', floor_info)
                    if floor_match:
                        permit_data['floors'] = int(floor_match.group(1))
                    
                    building_match = re.search(r'(\d+)æ£Ÿ', floor_info)
                    if building_match:
                        permit_data['buildings'] = int(building_match.group(1))
                    
                    unit_match = re.search(r'(\d+)æˆ¶', floor_info)
                    if unit_match:
                        permit_data['units'] = int(unit_match.group(1))
        
        # è§£æç¸½æ¨“åœ°æ¿é¢ç©
        area_match = re.search(r'ç¸½æ¨“åœ°æ¿é¢ç©.*?<span class="conlist w50">([0-9.,]+)', response.text)
        if area_match:
            permit_data['totalFloorArea'] = float(area_match.group(1).replace(',', ''))
        
        # è§£æç™¼ç…§æ—¥æœŸ
        date_match = re.search(r'ç™¼ç…§æ—¥æœŸ.*?(\d{4})/(\d{2})/(\d{2})', response.text)
        if date_match:
            year_date = int(date_match.group(1))
            month = int(date_match.group(2))
            day = int(date_match.group(3))
            permit_data['issueDate'] = f"{year_date}-{month:02d}-{day:02d}"
        
        return permit_data
        
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±æ•— {index_key}: {e}")
        return None

def main():
    """ä¸»ç¨‹å¼"""
    print(f"ğŸš€ Google Cloud æ‰‹å‹•çˆ¬èŸ²é–‹å§‹: {datetime.now()}")
    
    # æª¢æŸ¥ IP
    if not check_taiwan_ip():
        print("âš ï¸  è­¦å‘Šï¼šç›®å‰ä¸æ˜¯å°ç£ IPï¼Œå¯èƒ½ç„¡æ³•æ­£å¸¸çˆ¬å–")
        print("   å»ºè­°ä½¿ç”¨ Google Cloud å°ç£å€åŸŸçš„å¯¦ä¾‹åŸ·è¡Œ")
    
    # è¼‰å…¥ç¾æœ‰è³‡æ–™
    permits_data = []
    
    # å˜—è©¦å¾ GCS ä¸‹è¼‰
    if download_from_gcs("permits.json", LOCAL_DATA_FILE):
        with open(LOCAL_DATA_FILE, "r", encoding="utf-8") as f:
            permits_data = json.load(f)
        print(f"ğŸ“Š å¾ GCS è¼‰å…¥è³‡æ–™: {len(permits_data)} ç­†")
    elif os.path.exists(LOCAL_DATA_FILE):
        # ä½¿ç”¨æœ¬åœ°æª”æ¡ˆ
        with open(LOCAL_DATA_FILE, "r", encoding="utf-8") as f:
            permits_data = json.load(f)
        print(f"ğŸ“Š ä½¿ç”¨æœ¬åœ°è³‡æ–™: {len(permits_data)} ç­†")
    
    # å–å¾—æœ€æ–°åºè™Ÿ
    latest_seq = 0
    year_114_count = 0
    for permit in permits_data:
        if permit.get('year') == 114:
            year_114_count += 1
            seq = permit.get('sequence', 0)
            if seq > latest_seq:
                latest_seq = seq
    
    print(f"ğŸ“Š 114å¹´è³‡æ–™: {year_114_count} ç­†")
    print(f"ğŸ“Š æœ€æ–°åºè™Ÿ: {latest_seq}")
    
    # è©¢å•çˆ¬å–ç¯„åœ
    print("\nè«‹é¸æ“‡çˆ¬å–æ¨¡å¼:")
    print("1. å¾æœ€æ–°åºè™Ÿé–‹å§‹çˆ¬å–")
    print("2. æŒ‡å®šåºè™Ÿç¯„åœ")
    print("3. åªæ¸¬è©¦ä¸€ç­†")
    
    choice = input("è«‹è¼¸å…¥é¸é … (1/2/3): ").strip()
    
    if choice == "1":
        start_seq = latest_seq + 1
        end_seq = start_seq + 50  # é è¨­çˆ¬50ç­†
    elif choice == "2":
        start_seq = int(input("èµ·å§‹åºè™Ÿ: "))
        end_seq = int(input("çµæŸåºè™Ÿ: "))
    else:
        start_seq = latest_seq + 1
        end_seq = start_seq + 1
    
    # é–‹å§‹çˆ¬å–
    new_count = 0
    blank_count = 0
    error_count = 0
    
    print(f"\né–‹å§‹çˆ¬å–: {start_seq} ~ {end_seq}")
    
    for seq in range(start_seq, end_seq):
        print(f"\nğŸ” [{seq - start_seq + 1}/{end_seq - start_seq}] çˆ¬å–: 114å¹´ åºè™Ÿ{seq}")
        
        result = crawl_single_permit(114, seq)
        
        if result == "BLANK":
            print("   âšª ç©ºç™½è³‡æ–™")
            blank_count += 1
            if blank_count >= 5:  # é€£çºŒ5ç­†ç©ºç™½å°±åœæ­¢
                print("\nâŒ é€£çºŒ5ç­†ç©ºç™½è³‡æ–™ï¼Œåœæ­¢çˆ¬å–")
                break
        elif result:
            # æª¢æŸ¥æ˜¯å¦é‡è¤‡
            exists = any(p.get('indexKey') == result.get('indexKey') for p in permits_data)
            if not exists:
                permits_data.append(result)
                print(f"   âœ… æˆåŠŸ: {result.get('permitNumber', 'N/A')} - {result.get('applicant', 'N/A')}")
                new_count += 1
            else:
                print(f"   âš ï¸  å·²å­˜åœ¨: {result.get('permitNumber', 'N/A')}")
            blank_count = 0  # é‡è¨­ç©ºç™½è¨ˆæ•¸
        else:
            print("   âŒ å¤±æ•—")
            error_count += 1
            if error_count >= 3:  # é€£çºŒ3æ¬¡éŒ¯èª¤å°±åœæ­¢
                print("\nâŒ é€£çºŒ3æ¬¡éŒ¯èª¤ï¼Œåœæ­¢çˆ¬å–")
                break
        
        time.sleep(1.5)  # å»¶é²1.5ç§’
    
    # å„²å­˜è³‡æ–™
    if new_count > 0:
        print(f"\nğŸ’¾ å„²å­˜è³‡æ–™...")
        with open(LOCAL_DATA_FILE, "w", encoding="utf-8") as f:
            json.dump(permits_data, f, ensure_ascii=False, indent=2)
        
        # ä¸Šå‚³åˆ° GCS
        upload_to_gcs(LOCAL_DATA_FILE, "permits.json")
        
        # å»ºç«‹ä¸¦ä¸Šå‚³åŸ·è¡Œè¨˜éŒ„
        log_data = {
            'executionTime': datetime.now().isoformat(),
            'newRecords': new_count,
            'totalRecords': len(permits_data),
            'latestSequence': latest_seq + new_count,
            'year114Count': sum(1 for p in permits_data if p.get('year') == 114)
        }
        
        log_file = f"logs/crawl_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open("crawl_log.json", "w", encoding="utf-8") as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)
        
        upload_to_gcs("crawl_log.json", log_file)
    
    print(f"\nğŸ“Š åŸ·è¡Œçµ±è¨ˆ:")
    print(f"   æ–°å¢: {new_count} ç­†")
    print(f"   ç¸½è¨ˆ: {len(permits_data)} ç­†")
    print(f"   114å¹´: {sum(1 for p in permits_data if p.get('year') == 114)} ç­†")
    print(f"\nğŸš€ çˆ¬èŸ²åŸ·è¡Œå®Œæˆ: {datetime.now()}")

if __name__ == "__main__":
    main()